[{"body":"For users who are new to KubeClipper and want to get started quickly, it is recommended to use the All-in-One installation mode, which can help you quickly deploy KubeClipper with zero configuration.\nDeploy KubeClipper Download kcctl KubeClipper provides a command line tool 🔧 kcctl to simplify operation and maintenance. You can download the latest version of kcctl directly with the following command:\n# The latest distribution is installed by default curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | bash - # Install the specified version curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=v1.3.1 bash - #If you are in China, you can use cn environment variables during installation, in this case we will use registry.aliyuncs.com/google_containers instead of k8s.gcr.io Curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn bash - You can also download the specified version from the [GitHub Release Page] ( https://github.com/kubeclipper/kubeclipper/releases ) .\nCheck if the installation was successful with the following command:\nKcctl version Start installation You can use ‘kcctl deploy’ to quickly install and deploy KubeClipper. kcctl uses SSH to access the target node where KubeClipper is finally deployed, so you need to provide SSH access credentials, and the following way to pass the credentials:\nKcctl deploy [--user \u003cusername\u003e] [--passwd \u003cpassword\u003e | --pk-file \u003cprivate key path\u003e] Example：\n# Use the private key kcctl deploy --user root --pk-file /root/.ssh/id_rsa # Use a password kcctl deploy --user root --passwd password Execute the ‘kcctl deploy’ command kcctl will check your installation environment and will automatically enter the installation process if the conditions are met. If you see the following KubeClipper banner, the installation is successful.\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| You can also deploy the master version of KubeClipper to experience the latest features (the master version is not rigorously validated and may contain unknown bugs that affect the experience)\nInstall the master version kcctl curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=master bash - Set environment variables on the installation server export KC_VERSION=master Deploy KubeClipper AIO environment kcctl deploy Login to console After the installation is complete, open a browser and visit ‘http:’ to enter the KubeClipper console. (Usually kc-server IP is the IP of the node where you deploy kubeClipper)\nYou can use the default account password \" admin/Thinkbig1 \" to log in.\nYou may need to configure port forwarding rules and open ports in security groups for external users to access the console.\nCreate kubernetes cluster After successful deployment you can create a kubernetes cluster using the ** kcctl tool ** or via the ** console ** . Use the kcctl tool to create it in this quickstart tutorial.\nFirst, use the default account password to log in and obtain the token, which is convenient for subsequent interaction between kcctl and kc-server.\nkcctl login -H http://\u003ckc-server ip address\u003e:8080 -u admin -p Thinkbig1 Then create a Kubernetes cluster with the following command:\nNODE = $ (kcctl get node -o yaml | grep ipv4DefaultIP: | sed's/ipv4DefaultIP : //') Kcctl create cluster --master $NODE --name demo --untaint-master It takes about 3 minutes to complete the cluster creation, or you can use the following command to view the cluster status\nKcctl get cluster -o yaml | grep status -A5 You can also go to the console to view the real-time log.\nThe cluster installation is complete when the cluster is in the Running state, and you can use the ‘kubectl get cs’ command to view the cluster health.\n","categories":["QuickStart"],"description":"Deploying the AIO environment\n","excerpt":"Deploying the AIO environment\n","ref":"/v1.3/en/docs/getting-started/aio-env/","tags":["aio","sample","docs"],"title":"Deploying AIO"},{"body":"What is KubeClipper? KubeClipper aims to provide easy-to-use, easy-to-operate, lightweight, product-grade kubernetes multi-cluster full lifecycle management service, freeing operation and maintenance engineers from complicated configuration and obscure command lines to achieve one-stop management of multi-K8 s clusters across regions and infrastructure.\nWhy do I want KubeClipper? In the cloud-native era, Kubernetes has undoubtedly become the de facto standard for container orchestration. Although there are many tools to assist in the installation and management of kubernetes clusters, it is still very complicated to build and operate a production-level kubernetes cluster. In the process of a large number of services and practices, 99cloud has precipitated an extremely lightweight and easy-to-use graphical interface Kubernetes multi-cluster management tool - KubeClipper.\nUnder the premise of being fully compatible with native Kubernetes, KubeClipper is repackaged based on the kubeadm tool widely used by the community, providing rapid deployment of kubernetes clusters and continuous full life cycle management (installation, uninstallation, upgrade, scaling) in the enterprise’s own infrastructure. It supports multiple deployment methods such as online, proxy, and offline, and also provides rich and scalable management services for CRI, CNI, CSI, and various CRD components.\nCompared with the existing kubernetes lifecycle management tools such as Sealos, KubeKey, Kubeasz, KubeOperator, and K0S, KubeClipper is more open and native, lightweight, convenient, stable and easy to use.\nGetting Started: Get started with $project Examples: Check out some example code! ","categories":"","description":"Manage kubernetes in the most light and convenient way ☸️\n","excerpt":"Manage kubernetes in the most light and convenient way ☸️\n","ref":"/v1.3/en/docs/overview/","tags":"","title":"Overview"},{"body":"KubeClipper 是什么？ KubeClipper 旨在提供易使用、易运维、极轻量、生产级的 Kubernetes 多集群全生命周期管理服务，让运维工程师从繁复的配置和晦涩的命令行中解放出来，实现一站式管理跨区域、跨基础设施的多 kubernetes 集群。\n为什么需要 KubeClipper？ 云原生时代，Kubernetes 已毋庸置疑地成为容器编排的事实标准。虽然有诸多辅助 kubernetes 集群安装和管理的工具，但搭建和运维一套生产级别的 kubernetes 集群仍然十分复杂。九州云在大量的服务和实践过程中，沉淀出一个极轻量、易使用的图形化界面 Kubernetes 多集群管理工具—— KubeClipper。\nKubeClipper 在完全兼容原生 Kubernetes 的前提下，基于社区广泛使用的 kubeadm 工具进行二次封装，提供在企业自有基础设施中快速部署 kubernetes 集群和持续化全生命周期管理（安装、卸载、升级、扩缩容、远程访问等）能力，支持在线、代理、离线等多种部署方式，还提供了丰富可扩展的 CRI、CNI、CSI、以及各类 CRD 组件的管理服务。\n与现有的 Sealos、KubeKey、Kubeasz、KubeOperator、K0S 等 kubernetes 生命周期管理工具相比，KubeClipper 更贴近开放原生、轻量便捷、稳定易用。\n快速开始: Learn how to get started with Kubeclipper 用户手册: Check out some example code! KubeClipper 架构设计 核心架构 KubeClipper 分为三个部分：\nkc-server：原则上部署在独立节点，负责收集节点上报信息，分发前端操作任务至指定 kc-agent 并汇总执行结果等，是 KubeClipper 控制核心。 kc-agent: 部署在纳管节点，通过消息队列（内置 nats）与 kc-server 通信，负责上报节点信息以及处理下发任务并执行，是 KubeClipper 节点代理工具。 kcctl: KubeClipper 的终端命令行工具，可快捷高效的部署、管理 KubeClipper 集群，能够替代大多数页面操作。 节点纳管 部署网络模型 KubeClipper 支持可以通过参数配置来部署分层网络模型，以下是不同网络的概要：\n运维管理网络(Management Network):\n在 kcctl deploy 部署 KubeClipper 集群时，可通过 --ip-detect 参数指定网卡接口，默认为 first-found。 该网卡接口对应的 IP 地址即是 kc-server 与 kc-agent 的路由地址。\nK8S 主机网络(K8S Host Network):\n在 kcctl deploy 部署 KubeClipper 集群时，可通过 --node-ip-detct 参数指定网卡接口，默认继承 --ip-detect 参数值，亦可独立设置。 该网卡接口对应的 IP 地址即是 K8S 的节点路由地址。\nK8S Pod 网络(SDN Pod Network):\n在安装 K8S 集群时，可通过填入 CNI 的 POD 网路底层 指定网卡接口，默认为 first-found。 该网卡接口对应的 IP 地址即是 K8S POD 网路底层的路由地址。\n","categories":"","description":"Manage kubernetes in the most light and convenient way ☸️\n","excerpt":"Manage kubernetes in the most light and convenient way ☸️\n","ref":"/v1.3/docs/overview/","tags":"","title":"概述"},{"body":"Prepare to create a cluster You need to have enough available nodes. To add nodes, refer to \"Add Nodes\".\nPrepare the image or binary files of kubernetes, CRI, calico, CSI and other plug-ins that need to be installed. You can choose online/offline according to the network environment of the platform, and choose the recommended kubernetes version on page. You can also upload the image required for deployment to your own image repository in advance, and specify the image repository during deployment. For more installation configuration, refer to \"Cluster Configuration Guide\".\nCreate an AIO experimental cluster Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, and click the \"Create Cluster\" button in the upper left corner.\nEnter the \"Node Config\" page of the Create Cluster Wizard page. Fill in the \"Cluster Name\", such as \"test\", without selecting \"Cluster Template\". Select an available node, add it as a master node, and remove the taint from the master node in the taints list. Click the \"Next\" button.\nEnter the \"Cluster Config\" page. Select \"Offline\" for “Image Type”, retain the default values for other configurations, click the \"Create Quickly\" button, jump to the “Confirm Config” page, and click the \"Confirm\" button.\nThe experimental cluster of a single node is created. You can view the cluster details on the cluster details page, or click the \"ViewLog\" button to view the real-time log during the cluster creation process.\nCreate a cluster using a private registry If you create a cluster that contains large images, it is recommended that you upload the requred images to a private registry to speed up the installing process.\nAdd a private registry. Click \"Cluster Management\" \u003e \"Registry\" to enter the registry list page, and click the \"Add\" button in the upper left corner. In the pop-up window, enter the name and address of the registry where the images are stored, and click the \"OK\" button.\nCreate a cluster. Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, and click the \"Create Cluster\" button in the upper left corner. Configure the cluster nodes as needed. In the \"Private Registry\" of the \"Cluster Config\" page, select the registry added in the first step, and create the cluster after completing other configurations of the cluster as needed.\nCreate a cluster using the cluster template You can use cluster templates to simplify the cluster creation process.\nAdd a template. There are two ways to save a template. You can add a cluster template on the \"Cluster Management\" \u003e \"Template Management\" page, and select the template when creating a new cluster. You can also save the existing cluster configuration as a template by clicking \"More\" \u003e\"Cluster setings\"\u003e \"Save as Template\" in the cluster operation, so as to create a kubernetes cluster with the same configuration as the former cluster.\nCreate a cluster. Click \"Cluster Management\" \u003e \"Cluster\" to enter the cluster list page, click the \"Create Cluster\" button in the upper left corner, enter the cluster creation page, fill in the \"cluster name\", such as \"demo\", select the cluster template saved in the first step. Add the required nodes, click the \"Create Quickly\" button in the lower right corner, jump to the \"Confirm Config\" page, after checking the template information, click the \"Confirm\" button to create a cluster.\nCluster Configuration Guide Node configuration On the node config page, you can configure the node as follows:\nRegion: The region to which the cluster belongs. When adding a node, a physical or logical region can be specified for the node. The kubernetes cluster created by the node under this region also belongs to this region. Creating a cluster with nodes from multiple regionals is not supported.\nMaster Nodes: Specify an odd number of master nodes for the cluster. The production environments generally use 3 master nodes to achieve high availability.\nWorker nodes: Add worker nodes to the new cluster according to the business size.\nTaint management: You can configure taint for added nodes, kubeclipper will automatically add noschedule taint to the master nodes, and you can also make changes as needed.\nNode Labels: You can configure labels for added cluster nodes as needed.\nYou can configure the required nodes according to your business needs. If you need to create a non-highly available experimental cluster, you can also add only one master node, and remove the taint automatically added for the master node. For details, refer to \"Creating an AIO Experimental Cluster\".\nCluster configuration On the cluster configuration page, you can configure the cluster as follows:\nInstallation method and registry: Online: public network environment Offline: intranet environment no private registry Specified private registry Online Configuration package: Download from kubeclipper.io. Images: The image is pulled from the official registry by default, for example, kubernetes image pulled from k8s.gcr.io, calico pulled from docker.io. Configuration package source: Download from kubeclipper.io.\nImages: Pulled from the filled private registry. The components will inherit the registry by default. Please ensure that the required images are stored in the registry. You can also set an independent registry for a specific component, and the component image will be pulled from this registry. Offline Configuration package: Download from the local kubeclipper server nodes, you can use the “kcctl resource list” command to check the available configuration packages, or use the “kcctl resource push” command to upload the required configuration packages.\nImages: Download from the local kubeclipper server nodes, and CRI will import the images after downloading. You can use the “kcctl resource list” command to check the available image packages, or use the “kcctl resource push” command to upload the required image packages. Configuration package: Download from the local kubeclipper server nodes, you can use the “kcctl resource list” command to check the available configuration packages, or use the “kcctl resource push” command to upload the required configuration packages.\nImages: Pulled from the filled private registry. The components will inherit the registry by default. Please ensure that the required images are stored in the registry. You can also set an independent registry for a specific component, and the component image will be pulled from this address. kubeclipper provides the Docker Registry and uses the “kcctl registry” command for management. You can also use your own private registry. kubernetes version: Specify the cluster kubernetes version. When you choose to install offline, you can choose from the kubernetes version of the configuration packages in the current environment; when you choose to install online, you can choose from the officially recommended versions by kubeclipper.\nETCD Data Dir: You can specify the ETCD data directory, which defaults to /var/lib/etcd.\nkubelet Data Dir: You can specify the ETCD data directory, which defaults to /var/lib/kubelet.\nCertSANs: The IP address or domain name of the kubernetes cluster ca certificate signature, more than one can be filled in.\nContainer Runtime: According to the specified kubernetes version, the default container runtime is Docker for kubernetes version before v1.20.0, the default container runtime is Contianerd after v1.20.0; Docker is not supported after v1.24.0.\nContainer Runtime version: Specify the containerd/docker version. As with kubernetes, when you choose to install offline, you can choose from the version of the configuration package in the current environment; when you choose to install online, you can choose from the officially recommended version by kubeclipper.\nContainerd data Path: The \"root dir\" in the config.toml configuration can be filled in. which defaults to /var/lib/containerd.\nDocker data Path: The \"root dir\" in the daemon.json configuration can be filled in . which defaults to /var/lib/docker.\nContainerd registry: The registry address where the images are stored, the \"registry.mirrors\" in the config.toml configuration, more than one can be filled in.\nDocker registry: The registry address where the images are stored, the insecure registry in the daemon.json configuration, more than one can be filled in.\nDNS domain name: The domain name of the kubernetes cluster, which defaults to cluster.local.\nWorker load IP: Used for load balancing from worker nodes to multiple masters, no need to be set for a single master node cluster.\nExternal access IP: You can fill in a floating IP for user access, which can be empty.\nBackup space: Storage location of cluster backup files.\nCNI configuration The current version kubeclipper supports Calico as cluster CNI.\nCalico divides the pod CIDR set by users into several blocks (network segments), dynamically allocates them to the required nodes according to business requirements, and maintains the routing table of the cluster nodes through the bgp peer in the nodes.\nFor example: container address pool: 172.25.0.0/16, dynamically allocated network segment pool: 172.25.0.0 - 172.25.255.192 (172.25.0.0/26 i.e. 10 bits), the number of dynamically allocated network segments: 1023, the number of pods per network segment: 61 (193-254), the total number of pods is 1023 * 61 = 62403, the relative maximum number of nodes (according to the 200 service pod as the reference value): 312.\nClusters larger than 50 nodes are currently not recommended. Clusters larger than 50 nodes are recommended to manually configure route reflection to optimize the stability of routing table maintenance for nodes in the cluster.\nTo use Calico as the cluster CNI, you need the following configuration:\nCalico mode: 5 network modes are supported: Overlay-IPIP-All: Use IP-in-IP technology to open up the network of pods of different nodes. Usually, this method is used in the environment where the underlying platform is IaaS. Of course, if your underlying network environment is directly a physical device, it is also completely can be used, but the efficiency and flexibility will be greatly reduced. It should be noted that you need to confirm that the underlying network environment (underlay) supports the IPIP protocol. (The network method using overlay will have a certain impact on network performance). Overlay-Vxlan-All: Use IP-in-IP technology to open up the network of pods of different nodes. Usually, this method is used in the environment where the underlying platform is IaaS. Of course, if your underlying network environment is directly a physical device, it is also completely can be used, but the efficiency and flexibility will be greatly reduced. In theory, it can run on any network environment. Usually, we will use it when the underlying environment does not support the IPIP protocol. (The network method using overlay has a certain impact on network performance). BGP : Use IP-in-IP technology to open up the network of pods of different nodes. Usually this method is used in a bare metal environment. Of course, if the Iaas platform supports BGP, it can also be used. In this mode, the IP communication of pods is accomplished by exchanging routing tables among nodes in the cluster. If you need to manually open up the pod network between multiple clusters, you need to pay attention that the addresses you assign should not conflict. Overly-IPIP-Cross-Subnet: Use IP-in-IP technology to open up the network of pods of different nodes. Usually this method is used in the environment where the underlying platform is IaaS . It should be noted that you need to confirm the underlying network environment (underlay) supports the IPIP protocol. The difference with Overlay-IPIP-All is that if two upper Pods of different nodes in the same network segment communicate with each other through the routing table, the efficiency of upper Pods of different nodes in the same network segment can be improved. Overly-Vxlan-Cross-Subnet: The logic is similar to that of Overly-IPIP-Cross-Subnet. IP version: The IP version can be specified as IPV4 or IPV4 IPV6 dual stack. Service subnet: Fill in the service subnet CIDR, v4 defaults to: 10.96.0.0/16, v6 defaults to fd03::/112, note that the Service network must not overlap with any host network. Pod CIDR: Fill in the pod subnet CIDR, v4 default: 172.25.0.0/24, v6 default is fd05::/120, note that the Pod network must not overlap with any host network. The bottom layer of the pod network: First-found (default): The program will traverse all valid IP addresses (local, loop back, docker bridge, etc. will be automatically excluded) according to ipfamily (v4 or v6). Usually, if it is a multi-network interface card, it will exclude the default gateway. The network interface card ip other than the gateway will be used as the routing address between nodes. Can-reach: Set the routing address between nodes by checking the reachability of the domain names or IP addresses. Interface: Get all network interface card device names that satisfy the regular expression and return the address of the first network interface card as the routing address between nodes. MTU: Configure the maximum transmission unit (MTU) for the Calico environment. It is recommended to be no larger than 1440. The default is 1440. See https://docs.projectcalico.org/networking/mtu for details. Storage configuration The current version of Kubeclipper supports NFS as external storage types.\nConnect to NFS storage For NFS type external storage, you need to set the following:\nField Function description description/optional ServerAddr ServerAddr, the service address of NFS Required SharedPath SharedPath, the service mount path for NFS Required StorageClassName StorageClassName, the name of the storage class The default is nfs-sc, the name can be customized, and it cannot be repeated with other storage classes in the cluster ReclaimPolicy ReclaimPolicy, VPC recovery strategy Delete/Retain ArchiveOnDelete ArchiveOnDelete, whether to archive PVC after deletion Yes/No MountOptions MountOptions, the options parameter of NFS, such as nfsvers = 4.1 Optional, you can fill in several Replicas Replicas, number of NFS provisioners Default is 1 After setting up the external storage, the card below will show the storages you have enabled. You can choose a storage class as the default storage. For PVCs that do not specify a specific StorageClass, the default storage class will be used.\nConfiguration Confirm You can check the cluster configuration information on the Confirm Config page. After confirming the information, click Confirm. You can also click the “Edit” button of each card to skip back to the corresponding step to modify the cluster information.\nThe cluster installation may take several minutes. You can check the operation logs on the cluster detail page to track the cluster installation status.\n","categories":"","description":"KubeClipper supports the creation of kubernetes clusters via a wizard-style page.\n","excerpt":"KubeClipper supports the creation of kubernetes clusters via a wizard-style page.\n","ref":"/v1.3/en/docs/tutorials/create-clusters/","tags":"","title":"Create clusters"},{"body":"创建集群准备工作 您需要准备充足的可用节点，如需添加节点，参见“添加节点”教程。 准备好需要部署的 kubernetes、CRI、calico、CSI 和其他插件的镜像或二进制文件，kubeclipper 提供了推荐的版本，您可以根据平台所处网络环境，选择在线 / 离线后，直接在页面上选取使用。您也可以将部署所需的镜像上传至自己的镜像仓库，并在部署时指定。更多安装配置，参考“集群配置”章节。 创建单节点实验集群 点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮。\n进入创建集群向导页面的“节点配置”页面。填写“集群名称”，如 “test”，不需选择“集群模版”。选择一个可用节点，添加为控制节点，并在污点管理列表中，将 master 节点的污点移除。点击“下一步”按钮。\n进入创建集群向导页面的“集群配置”页面。选择“离线安装”，“镜像仓库”不需填写，其他配置都可使用默认配置，点击“快速创建”按钮，跳转配置确认页面，点击“确认”按钮。\n单节点的实验集群创建完成，您可以在集群详情页查看集群详情，也可以点击“查看日志”按钮，查看集群创建过程中的实时日志。\n使用镜像仓库创建集群 如果创建的集群中包含较大的镜像，推荐您将所有镜像上传到特定的镜像仓库，创建集群会更快速更顺畅。\n添加镜像仓库。点击“集群管理”\u003e“镜像仓库”，进入镜像仓库列表页面，点击左上角“添加”按钮。在添加镜像仓库的弹窗中输入镜像仓库名称和存放有镜像的仓库地址，点击“确定”按钮。 创建集群。点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮。按需配置集群节点，在“集群配置”页面的“镜像仓库”中，选择第一步添加的镜像仓库，根据需要完成集群其他配置后创建集群。 使用集群模版创建集群 您可以使用集群模版，简化集群创建流程。\n添加模版。保存模版有两种方式，您可以在“集群管理”\u003e“模版管理”页面，添加集群模版，以备创建集群时使用。也可以点击集群操作中的“更多”\u003e“保存为模版”，将已存在的集群配置保存为模版，以便创建出和该集群同等配置的 kubernetes 集群。 创建集群。点击“集群管理”\u003e“集群”，进入集群列表页面，点击左上角“创建集群”按钮，进入创建集群页面，填写“集群名称”，如 “demo”，选择第一步中保存的集群模版，添加所需节点，点击右下角“快速创建”按钮，跳转至“配置确认”页面，核对模版信息无误后，点击“确认”按钮，创建集群。 集群配置指南 节点配置 在节点配置页面，您可以对节点进行以下配置：\n区域：集群所属区域，添加节点时可为节点指定物理的或逻辑的区域，使用该区域下节点创建的 kubernetes 集群也属于该区域，不支持使用跨区域的多个节点创建集群。 控制节点：为集群指定奇数个的控制节点，生产环境一般使用3个控制节点以实现高可用。 工作节点：根据业务规模，为新集群添加工作节点。 污点管理：您可以为已添加的节点配置污点，kubeclipper 会自动为控制节点添加不允许调度（noschedule）的污点，您也可以根据需要进行更改。 节点标签：您可以根据需要为已添加的集群节点配置标签。 您可以按业务需要配置所需节点。如果需要创建非高可用的实验集群，也可以仅添加一个控制节点，并将控制节点自动添加的污点移除，详细操作参见“创建单节点实验集群”。\n集群配置 在集群配置页面，您可以对集群进行以下配置：\n安装方式和镜像仓库： 镜像仓库为空 指定镜像仓库 在线（公网环境） 配置包来源：从 kubeclipper.io 下载。\n镜像拉取方式：镜像默认从官方镜像仓库拉取，如 kubernetes 镜像从 k8s.gcr.io 拉取、calico 从 docker.io 拉取。如果您设置了国内镜像代理，镜像会从您指定的 “KC_IMAGE_REPO_MIRROR” 代理仓库拉取。 配置包来源：从 kubeclipper.io 下载。\n镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。 离线（内网环境） 配置包来源：从本地 kubeclipper 集群 server 节点下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n镜像拉取方式：从本地 kubeclipper 集群 server 节点下载，下载后由 CRI 进行镜像导入。您可以使用 kcctl resource list 命令查看本地可用镜像包，或使用 kcctl resource push 命令上传所需镜像包。 配置包来源：从本地下载，您可以使用 kcctl resource list 命令查看本地可用配置包，或使用 kcctl resource push 命令上传所需配置包。\n镜像拉取方式：从填写的镜像仓库拉取，组件将默认继承该仓库地址，请确保该仓库存在相关组件镜像；组件也会提供独立的镜像仓库参数，设置后组件镜像从该地址拉取。kubeclipper 提供 Docker Registry 方案，并使用 kcctl registry 命令行进行管理，您也可以使用其他自有镜像仓库。 kubernetes 版本：指定集群 kubernetes 版本。当您选择离线安装的时候，可以从当前环境中配置包的 kubernetes 版本中选择；当您选择在线安装的时候，可以从 kubeclipper 官方推荐的版本中选择。\nETCD 数据目录：可指定 ETCD 数据目录，默认为/var/lib/etcd。\nkubelet 数据目录：可指定 kubelet 数据目录，默认为 /var/lib/kubelet。\nCertSANs：kubernetes 集群 ca 证书签名的 ip 或者域名，可填写多个。\n容器运行时：根据指定 kubernetes 版本，kubernetes 版本在 v1.20.0 之前，容器运行时默认 docker，之后默认 containerd；v1.24.0 之后不支持 docker。\n容器运行时版本：指定 containerd / docker 版本。与 kubernetes 相同，当您选择离线安装的时候，可以从当前环境中配置包的版本中选择；当您选择在线安装的时候，可以从 kubeclipper 官方推荐的版本中选择。\nContainerd 数据目录：可填写 config.toml 配置中的 root dir，默认为/var/lib/containerd。\nDocker 数据目录：可填写 daemon.json 配置中的 root dir，默认为/var/lib/docker。\nContainerd 镜像仓库：存放 containerd 镜像的仓库地址，config.toml 配置中的 registry.mirrors，可填写多个。\nDocker 镜像仓库：存放 docker 镜像的仓库地址，daemon.json 配置中的 insecure registry,可填写多个。\nDNS 域名：kubernetes 集群的域名，默认为 cluster.local。\nWorker 负载 IP：用于 worker 节点到多 master 的负载均衡，单一 master 不需要设置。\n外部访问 IP：可以填写一个浮动 IP 给用户访问，可为空。\n备份空间：集群备份文件存储位置。\nCNI 配置 当前版本仅支持 Calico 作为集群 CNI。\nCalico 将用户设置的 pod cidr 分为若干个 block (网段)，根据业务需求动态的分配给需要的节点，并在节点中通过 bgp peer 维护集群节点的路由表。\n例如：容器的地址池：172.25.0.0/16，动态分配的网段池: 172.25.0.0 - 172.25.255.192 (172.25.0.0/26 即 10 个比特位)，动态分配的网段数: 1023，每个网段的 pod 数量为: 61 (193-254)，总 pod 数量为1023 * 61 = 62403，相对最大节点数(按照200业务 pod 为基准值)：312。\n目前不建议大于50个节点的集群，大于50个节点的集群建议手动配置 route reflection，用来优化集群中的节点的路由表维护的稳定性。\n使用 Calico 作为集群 CNI，您需要进行以下配置：\nCalico 模式：支持5种网络模式：\nOverlay-IPIP-All: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,当然如果你底层网路环境直接是物理设备的也完全可以使用只不过效率和灵活度都会大打折扣,需要注意的是你需要确认底层网络环境(underlay)是支持 IPIP 协议的.(使用 overlay 的网络方式对网络性能造成一定的影响)。 Overlay-Vxlan-All: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,当然如果你底层网路环境直接是物理设备的也完全可以使用只不过效率和灵活度都会大打折扣,他理论上可以在任何的网络环境上运行,通常在底层环境不支持 IPIP 协议的时候我们会使用他.(使用 overlay 的网络方式对网络性能造成一定的影响)。 BGP: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在裸机的环境上,当然底 Iaas 平台支持 BGP 的话也是可以使用的,这种模式下 pod 的 ip 通信是通过 集群中的各个节点中互相交换路由表来完成 pod 之间的通信的,如果你需要手动打通多个集群之间的 pod 网络需要注意你分配的地址断不应该有冲突。 Overaly-IPIP-Cross-Subnet: 使用 IP-in-IP 技术打通不同节点的 pod 的网络,通常这样的方式使用在底层平台是 iaas 的环境之中,需要注意的是你需要确认底层网络环境(underlay)是支持 IPIP 协议的.和 Overlay-IPIP-All 的不同之处在于,如果 2 个不同节点但在同一个网段中的上 pod 互相通信时是通过路由表,这样可以提高在不同节点但在同一个网段中的上 pod 互相通信时的效率。 Overaly-Vxlan-Cross-Subnet: 和 Overaly-IPIP-Cross-Subnet 逻辑相似不再做重复的解释。 Calico 版本：指定 calico 版本。与 kubernetes 相同，当您选择离线安装的时候，可以从当前环境中配置包的版本中选择；当您选择在线安装的时候，可以从 kubeclipper 官方推荐的版本中选择。\nIP版本：可指定 IP 版本为 IPV4 或 IPV4 IPV6 双栈。\n服务子网：填写 service 子网 CIDR，v4 默认为：10.96.0.0/16，v6 默认为 fd03::/112，注意 Service 网络不得与任何主机网络重叠。\nPod CIDR：填写 Pod 子网 CIDR，v4 默认：172.25.0.0/24，v6 默认为 fd05::/120，注意 Pod 网络不得与任何主机网络重叠。\npod网路的底层：\nfirst-found（默认）：程序会根据 ipfamily (v4 或 v6)遍历所有的有效的 ip 地址（local,loop back，docker bridge等会被自动排除）通常如果是多网卡时会排除默认网关以外的网卡的 ip 作为节点之间的路由地址。 can-reach：通过检查域名或者 ip 的可达性来设置节点之间的路由地址。 interface：根据正则表达式获取所有满足的网卡设备名称并返回第一个满足表达式网卡的地址作为节点之间的路由地址。 MTU：为 Calico 环境配置最大传输单元(MTU)，建议不大于1440，默认为1440，详情见 https://docs.projectcalico.org/networking/mtu。\n存储配置 Kubeclipper当前版本内置了 NFS 作为集群外接存储。\n对接 NFS 类型的外接存储，您需要设置以下内容：\n字段 作用说明 填写说明/可选项 服务地址 ServerAddr，NFS的服务地址 必填 共享路径 SharedPath，NFS的服务挂载路径 必填 存储类 StorageClassName，存储类的名称 默认为 nfs-sc，可自定义名称，不可与集群其他存储类重复 回收策略 ReclaimPolicy，VPC回收策略 删除 Delete / 保留 Retain 挂载选项 MountOptions，NFS 的 options 参数，如nfsvers=3 选填，可填写多个 副本数 Replicas，NFS provisioner副本数 默认为1 设置完外接存储后，下方卡片会显示您已经开启的存储，您可以选择一个存储类作为默认存储，对于未指定特定StorageClass 的 PVC ，会直接使用默认的存储类。\n配置确认 您可以在配置确认页面浏览集群的配置信息，确认无误后，点击“确认“按钮。也可以点击卡片右侧的“编辑”按钮，跳回到相应步骤修改集群信息。\n安装集群可能需要较长时间，您可以在集群详情页面查看操作日志，跟踪集群安装状态。\n设置国内镜像代理（可选） Q: 设置国内镜像代理有什么作用？\nA: 设置了国内镜像代理，使用在线安装功能时，会从指定代理拉取 kubernetes 相关镜像，避免在国内无法访问 gcr 而导致集群安装失败问题。\n以下设置方法需在执行 kcctl deploy 部署命令的机器上完成，任选其一即可。\n**方式一：**设置 KC_IMAGE_REPO_MIRROR 环境变量（推荐）\n# 设置环境变量，推荐使用阿里云镜像代理，您也可以自行设定 export KC_IMAGE_REPO_MIRROR=\"registry.aliyuncs.com/google_containers\" **方式二：**设置 /etc/kc/kc.env 环境变量文件\n# 创建 kc 目录 mkdir -pv /etc/kc # 创建 kc.env 文件并传入环境变量信息 cat \u003c\u003cEOF \u003e /etc/kc/kc.env KC_IMAGE_REPO_MIRROR=\"registry.aliyuncs.com/google_containers\" EOF ","categories":"","description":"KubeClipper 支持通过向导式页面创建 kubernetes 集群，并安装 CNI、CSI 等所需插件。\n","excerpt":"KubeClipper 支持通过向导式页面创建 kubernetes 集群，并安装 CNI、CSI 等所需插件。\n","ref":"/v1.3/docs/tutorials/create-clusters/","tags":"","title":"创建集群"},{"body":" For the first contact with KubeClipper, it is recommended to deploy AIO environment and quickly get started to experience the features provided by KubeClipper. If you want to apply KubeClipper to a build environment, then this document may be helpful.\nOverview According to the KubeClipper architecture design, KubeClipper has the following 4 core components:\nKc-server: mainly includes APISERVER, controller, static resource services and built-in message queue, etc., kc-server communicates with kc-agent through message queue (supports external); kc-server has no master-slave relationship and is independent of each other; usually deployed in independent nodes to provide stable and reliable services to the outside world. Kc-agent: mainly includes the task processor, which is responsible for receiving the tasks delivered by the kc-server and feeding back the task processing results; usually deployed in nodes that need to install kubernetes, it is an ultra-lightweight service process. Kc-etcd: The backend database of kc-server, deployed on the same node as kc-server. Kc-dashboard: graphical management interface, deployed on the same node with kc-server. To sum up, we call the node that deploys kc-server as server, and the node that deploys kc-agent as agent. Then the key point of deploying a highly available KubeClipper cluster is how to plan and deploy server nodes while ensuring the high availability of kc-etcd.\nGenerally speaking, for deploying highly available distributed application clusters, it is basically recommended to have at least 3 nodes; also for KubeClipper, 3 nodes can ensure that kc-server can still provide services after 2 nodes Downtime, and can ensure that kc-etcd will not appear Split-Brain exception.\nThe above brief introduction to the KubeClipper architecture and core components is to better understand how to deploy a highly available KubeClipper cluster, so as to lead to thinking about server node planning and Hardware configuration requirements.\nRecommended configuration KubeClipper as an extremely lightweight Kubernetes multi-cluster full lifecycle management tool, itself will not take up too many resources.\nserver node\nQuantity: 3 and more Hardware requirements: CPU \u003e = 2 cores, RAM \u003e = 2GB, hard disk \u003e = 20GB System: CentOS 7.x/Ubuntu 18.04/Ubuntu 20.04 Agent node\nQuantity: any Hardware requirements: according to actual needs System: CentOS 7.x/Ubuntu 18.04/Ubuntu 20.04 Start by installing kcctl Kcctl is a command line tool provided by KubeClipper that enables rapid deployment of KubeClipper clusters and most Kuberneters cluster management features to simplify operations.\nInstall kcctl:\n# The latest release is installed by default curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | bash - # Install the specified version curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=v1.3.1 bash - # If you are in China, you can specify the KC_REGION environment variable during installation, at this time we will use registry.aliyuncs.com/google_containers instead of k8s.gcr.io # This is very useful for online installation of k8s cluster curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn bash - After the installation is successful, the installation version and installation Path will be output.\nYou can also download the GitHub Release Page download the specified kcctl version\nVerify installation:\nkcctl version -o json kcctl version: { \"major\": \"1\", \"minor\": \"3\", \"gitVersion\": \"v1.3.1\", \"gitCommit\": \"5f19dcf78d3a9dc2d1035a779152fa993e0553df\", \"gitTreeState\": \"clean\", \"buildDate\": \"2022-12-02T10:12:36Z\", \"goVersion\": \"go1.19.2\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } # View help documentation kcctl -h Learn about the kcctl deploy command The kcctl deploy -h command is specially used to deploy KubeClipper cluster, for more examples and parameter explanation, please execute kcctl deploy -h\nIntroduction to common parameters\n–Server: IP server node, such as 192.168.10.10, 192.168.10.11, IP separated by commas. –Agent: Agent node IP, such as 192.168.10.10, 192.168.10.11, IP separated by commas. –Pk-file: ssh password-free login private key, it is recommended to use password-free login on the command line. –User: ssh login username, default is root. –Passwd: ssh login password, it is not recommended to use the password to log in at the command line. –Pkg: Installation package Path, support local Path and online link; get online installation package link rules: https://oss.kubeclipper.io/release/ {KC_VERSION}/kc- {GOARCH} .tar.gz . KC_VERSION for Release Version default setting current kcctl corresponding version, GOARCH is amd64 or arm64, default setting current kcctl Compilation architecture. –Ip-detect: Node IP discovery rules, support a variety of rules, such as specifying the name of the network interface card, etc., very useful for multiple network interface card nodes, the default is “first-found”. After understanding the basic usage of kcctl deploy, let’s start deploying the KubeClipper cluster.\nDeploy KubeClipper with kcctl We recommend that in the multi-node installation scenario, the server nodes involved are uniformly set up password-free login to avoid password Plain Text leakage.\nDeploy 3 server nodes with private key:\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --pkg https://oss.kubeclipper.io/release/{KC_VERSION}/kc-{GOARCH}.tar.gz Deploy 3 server + 3 agent nodes in private key mode, specify pkg:\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS \\ --pkg https://oss.kubeclipper.io/release/{KC_VERSION}/kc-{GOARCH}.tar.gz Deploy 3 server + 3 agent nodes with private key, pkg is not specified, and the default is the same as the installed version of kcctl (recommended):\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS Private key deployment 3 server + 3 agent node, specify etcd port, default port is client-12379 | peer-12380 | metrics-12381 :\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS \\ --etcd-port 12379 --etcd-peer-port 12380 --etcd-metric-port 12381 Parameter input example:\nSERVER_IPS: 192.168.10.20,192.168.10.21\nAGENT_IPS: 192.168.10.30,192.168.10.31\nKC_VERSION : KubeClipper release version, see GitHub Release Page\nGOARCH System Architecture, AMD64 (aka x84_64), ARM64 (aka AARCH 64)\nKcctl deploy supports a variety of parameters, which can meet your specific needs for deploying KubeClipper clusters, and more functions are waiting for you to explore.\nAfter executing the kcctl deploy command, the command will detect whether your environment meets the installation requirements, and will synchronize warning messages, installation progress, etc. to the Console. Finally, the following KubeClipper banner will be printed after the installation is successful:\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| System default management account: admin/Thinkbig1 Login to Console: Open a browser and visit http://SERVER_IP (accessible through any Server node) to enter the KubeClipper Console\nLogin command line:\nkcctl login -H http://SERVER_IP -u admin -p Thinkbig1 Most kcctl commands rely on login status, so it’s best to log in early when you execute the cli command.\nAdd agent node to KubeClipper using kcctl The current kcctl join command only supports adding agent nodes, and will gradually support adding server nodes in the future.\nNewly added agent nodes should also be uniformly set up password-free login, and the private key is the same.\nJoin agent node:\nkcctl join --agent=AGENT_IPS Remove agent node from KubeClipper using kcctl The current kcctl drain command only supports deleting agent nodes, and will gradually support deleting server nodes in the future.\nDrain agent node:\nkcctl drain --agent=AGENT_IPS # Force drain node, ignore errors kcctl drain --agent=AGENT_IPS --force If you find that KubeClipper cannot be successfully deployed according to this document, please move to the KubeClipper Github Issue to provide your comments or feedback.\n","categories":"","description":"Rapidly deploy highly available, production-ready KubeClipper clusters\n","excerpt":"Rapidly deploy highly available, production-ready KubeClipper clusters\n","ref":"/v1.3/en/docs/deployment-docs/ha-deploy/","tags":"","title":"Deploy Highly Available KubeClipper"},{"body":"","categories":["Examples","Placeholders"],"description":"Quickly build the experience platform function\n","excerpt":"Quickly build the experience platform function\n","ref":"/v1.3/en/docs/getting-started/","tags":["test","docs"],"title":"Getting Started"},{"body":"对于初次接触 KubeClipper 并想快速上手的用户，建议使用 All-in-One 安装模式，它能够帮助您零配置快速部署 KubeClipper。\n准备工作 KubeClipper 本身并不会占用太多资源，但是为了后续更好的运行 Kubernetes 建议硬件配置不低于最低要求。\n您仅需参考以下对机器硬件和操作系统的要求准备一台主机。\n硬件推荐配置 确保您的机器满足最低硬件要求：CPU \u003e= 2 核，内存 \u003e= 2GB。 操作系统：CentOS 7.x / Ubuntu 18.04 / Ubuntu 20.04。 节点要求 节点必须能够通过 SSH 连接。 节点上可以使用 sudo / curl / wget / tar 命令。 建议您的操作系统处于干净状态（不安装任何其他软件），否则可能会发生冲突。\n部署 KubeClipper 下载 kcctl KubeClipper 提供了命令行工具🔧 kcctl 以简化运维工作，您可以直接使用以下命令下载最新版 kcctl：\n# curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | bash - # 如果你在中国， 你可以在安装时使用 cn 环境变量, 此时我们会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn bash - 您也可以在 GitHub Release Page 下载指定版本。\n通过以下命令检测是否安装成功:\nkcctl version 开始安装 在本快速入门教程中，您只需执行一个命令即可安装 KubeClipper，其模板如下所示：\nkcctl deploy 若使用 ssh passwd 方式则命令如下所示:\nkcctl deploy --user root --passwd $SSH_PASSWD 私钥方式如下：\nkcctl deploy --user root --pk-file $SSH_PRIVATE_KEY 您只需要提供 ssh user 以及 ssh passwd 或者 ssh 私钥即可在本机部署 KubeClipper。\n执行该命令后，Kcctl 将检查您的安装环境，若满足条件将会进入安装流程。在打印出如下的 KubeClipper banner 后即表示安装完成。\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 登录控制台 安装完成后，打开浏览器，访问 http://$IP 即可进入 KubeClipper 控制台。\n您可以使用默认帐号密码 admin / Thinkbig1 进行登录。\n您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。\n创建 kubernetes 集群 部署成功后您可以使用 kcctl 工具或者通过控制台创建 kubernetes 集群。在本快速入门教程中使用 kcctl 工具进行创建。\n首先使用默认帐号密码进行登录获取 token，便于后续 kcctl 和 kc-server 进行交互。\nkcctl login -H http://localhost -u admin -p Thinkbig1 然后使用以下命令创建 kubernetes 集群:\nNODE=$(kcctl get node -o yaml|grep ipv4DefaultIP:|sed 's/ipv4DefaultIP: //') kcctl create cluster --master $NODE --name demo --untaint-master 大概 3 分钟左右即可完成集群创建,也可以使用以下命令查看集群状态\nkcctl get cluster -o yaml|grep status -A5 您也可以进入控制台查看实时日志。\n进入 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。\n","categories":["QuickStart"],"description":"快速搭建体验平台功能\n","excerpt":"快速搭建体验平台功能\n","ref":"/v1.3/docs/getting-started/","tags":["docs"],"title":"快速开始"},{"body":"对于初次接触 KubeClipper 并想快速上手的用户，建议使用 AIO（即 All-in-One，使用单个节点安装 KubeClipper）模式，它能够帮助您零配置快速部署 KubeClipper。\n部署 KubeClipper 下载并安装 kcctl KubeClipper 提供了命令行工具🔧 kcctl 以简化运维工作，您可以直接使用以下命令下载最新版 kcctl：\n# 默认安装最新的发行版 curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | bash - # 安装指定版本 curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=v1.3.1 bash - # 如果您在中国， 您可以在安装时使用 cn 环境变量, 此时 KubeClipper 会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn bash - 您也可以在 GitHub Release Page 下载指定版本。\n可以通过以下命令验证 kcctl 是否安装成功:\n# 如果一切顺利，您将看到 kcctl 版本信息 kcctl version 开始安装 您可以使用 kcctl deploy 快速安装部署 KubeClipper。kcctl 使用 SSH 访问最终部署 KubeClipper 的目标节点，因此需要您提供 SSH 访问凭证，传递凭证的方法如下：\nKcctl deploy [--user \u003cusername\u003e] [--passwd \u003cpassword\u003e | --pk-file \u003cprivate key path\u003e] 示例：\n# 使用私钥 kcctl deploy --user root --pk-file /root/.ssh/id_rsa # 使用密码 kcctl deploy --user root --passwd password 执行 kcctl deploy 命令 kcctl 将会检查您的安装环境，若满足条件将自动进入安装流程。若您看到如下 KubeClipper banner 后即表示安装成功。\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 您也可以部署 master 版本的 KubeClipper，来体验最新的功能特性（master 版本没有经过严格验证，可能包含影响体验的未知错误）\n安装 master 版本 kcctl curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=master bash - 在安装服务器上设置环境变量 export KC_VERSION=master 以 AIO 方式部署 KubeClipper kcctl deploy 登录控制台 安装完成后，打开浏览器，访问 http://\u003ckc-server ip address\u003e 即可进入 KubeClipper 控制台。(通常 kc-server ip 是您部署 kubeClipper 节点的 ip)\n您可以使用默认帐号密码 admin / Thinkbig1 进行登录。\n您可能需要配置端口转发规则并在安全组中开放端口，以便外部用户访问控制台。\n创建 Kubernetes 集群 部署成功后您可以使用 kcctl 工具或者通过控制台创建 Kubernetes 集群。在本快速入门教程中使用 kcctl 工具进行创建。\n首先使用默认帐号密码进行登录获取 token，便于后续 kcctl 和 kc-server 进行交互。\nkcctl login -H http://\u003ckc-server ip address\u003e:8080 -u admin -p Thinkbig1 通过以下命令创建 Kubernetes 集群:\nNODE=$(kcctl get node -o yaml|grep ipv4DefaultIP:|sed 's/ipv4DefaultIP: //') kcctl create cluster --master $NODE --name demo --untaint-master 大概 3 分钟左右即可完成集群创建,您可以使用以下命令查看集群状态\nkcctl get cluster -o yaml|grep status -A5 您也可以进入控制台查看实时日志。\n集群处于 Running 状态即表示集群安装完成,您可以使用 kubectl get cs 命令来查看集群健康状况。\n","categories":["QuickStart"],"description":"部署 AIO 环境\n","excerpt":"部署 AIO 环境\n","ref":"/v1.3/docs/getting-started/aio-env/","tags":["aio","sample","docs"],"title":"部署 AIO"},{"body":" 对于初次接触 KubeClipper，建议部署 AIO 环境，快速上手体验 KubeClipper 提供的功能特性。 对于想将 KubeClipper 应用到生成环境，那么本文档或许对您有所帮助。\n概述 根据 KubeClipper 架构设计可知，KubeClipper 有以下 4 个核心组件：\nkc-server：主要包括 APISERVER 、控制器、静态资源服务以及内置消息队列等，kc-server 通过消息队列（支持外置）与 kc-agent 通信；kc-server 之间无主从关系，且相互独立；通常部署在独立的节点，从而对外提供稳定可靠的服务。 kc-agent：主要包括任务处理器，负责接收 kc-server 投递的任务，并反馈任务处理结果；通常部署在需要安装 kubernetes 的节点，是一个超轻量级的服务进程。 kc-etcd：kc-server 的后端数据库，跟随 kc-server 部署在同一节点上。 kc-dashboard：图形化管理界面，跟随 kc-server 部署在同一节点。 综上，我们将部署 kc-server 的节点称为 server，部署 kc-agent 的节点称为 agent。 那么部署高可用 KubeClipper 集群的关键点，就在于如何规划部署 server 节点同时保证 kc-etcd 的高可用。 通常来看，对于部署高可用的分布式应用集群，基本建议节点至少 3 个；同样对于 KubeClipper，3 个节点能保证 kc-server 在其中 2 个节点宕机后依旧可以提供服务，同时能保证 kc-etcd 不会出现脑裂异常。\n以上简单介绍了 KubeClipper 架构以及核心组件，是为了更好的理解该如何部署高可用 KubeClipper 集群，从而引出关于对服务器节点规划以及硬件配置要求的思考。\n推荐配置 KubeClipper 作为一个极轻量的 Kubernetes 多集群全生命周期管理工具，本身不会占用太多资源\nserver 节点\n数量：3 个及以上 硬件要求：CPU \u003e= 2 核，内存 \u003e= 2GB，硬盘 \u003e= 20GB 系统：CentOS 7.x / Ubuntu 18.04 / Ubuntu 20.04 agent 节点\n数量：任意 硬件要求：依据实际需求而定 系统：CentOS 7.x / Ubuntu 18.04 / Ubuntu 20.04 从安装 kcctl 开始 kcctl 是 KubeClipper 提供的命令行工具，它支持快速部署 KubeClipper 集群以及大部分 Kuberneters 集群管理功能，用以简化运维工作。\n安装 kcctl：\n# 默认安装最新发行版 curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | bash - # 安装指定版本 curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=v1.3.1 bash - # 如果您在中国，您可以在安装时指定 KC_REGION 环境变量，此时我们会使用 registry.aliyuncs.com/google_containers 代替 k8s.gcr.io # 这对于在线安装 k8s 集群非常有用 curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn bash - 安装成功后，会输出安装版本以及安装路径等信息。\n您也可以在 GitHub Release Page 下载指定的 kcctl 版本\n验证安装：\nkcctl version -o json kcctl version: { \"major\": \"1\", \"minor\": \"3\", \"gitVersion\": \"v1.3.1\", \"gitCommit\": \"5f19dcf78d3a9dc2d1035a779152fa993e0553df\", \"gitTreeState\": \"clean\", \"buildDate\": \"2022-12-02T10:12:36Z\", \"goVersion\": \"go1.19.2\", \"compiler\": \"gc\", \"platform\": \"linux/amd64\" } # 查看帮助文档 kcctl -h 了解 kcctl deploy 命令 kcctl deploy 命令是专门用于部署 KubeClipper 集群，更多示例以及参数解释请执行 kcctl deploy -h\n常用参数简介\n–server: server 节点 IP，例如 192.168.10.10,192.168.10.11，多个 IP 以逗号隔开。 –agent: agent 节点 IP，例如 192.168.10.10,192.168.10.11，多个 IP 以逗号隔开。 –pk-file: ssh 免密登录私钥，推荐在命令行使用免密登录。 –user: ssh 登录用户名，默认为 root。 –passwd: ssh 登录密码，不推荐在命令行使用密码登录。 –pkg: 安装包路径，支持本地路径以及在线链接；获取在线安装包链接规则：https://oss.kubeclipper.io/release/{KC_VERSION}/kc-{GOARCH}.tar.gz 。KC_VERSION 为 Release Version 默认设置当前 kcctl 对应版本，GOARCH 为 amd64 或 arm64，默认设置当前 kcctl 的编译架构。 –ip-detect: 节点 ip 发现规则，支持多种规则，例如指定网卡名称等，对于多网卡节点非常有用，默认为 “first-found”。 了解完 kcctl deploy 的基础使用，那么接下来就开始部署 KubeClipper 集群吧。\n使用 kcctl 部署 KubeClipper 我们推荐在多节点安装场景中，将涉及到的服务器节点都统一设置免密登录，避免密码明文泄露。\n私钥方式部署 3 server 节点：\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --pkg https://oss.kubeclipper.io/release/{KC_VERSION}/kc-{GOARCH}.tar.gz 私钥方式部署 3 server + 3 agent 节点，指定 pkg：\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS \\ --pkg https://oss.kubeclipper.io/release/{KC_VERSION}/kc-{GOARCH}.tar.gz 私钥方式部署 3 server + 3 agent 节点，未指定 pkg，默认与 kcctl 安装版本一致（推荐）：\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS 私钥方式部署 3 server + 3 agent 节点，指定 etcd 端口，默认端口为 client-12379 | peer-12380 | metrics-12381：\nkcctl deploy --pk-file=~/.ssh/id_rsa \\ --server SERVER_IPS \\ --agent AGENT_IPS \\ --etcd-port 12379 --etcd-peer-port 12380 --etcd-metric-port 12381 参数输入示例：\nSERVER_IPS: 192.168.10.20,192.168.10.21\nAGENT_IPS: 192.168.10.30,192.168.10.31\nKC_VERSION: KubeClipper 的 release version，查看 GitHub Release Page 获取\nGOARCH：系统架构，amd64 （又名 x84_64），arm64（又名 aarch64）\nkcctl deploy 支持多种参数，能够满足您对部署 KubeClipper 集群的特定需求，更多功能等您探索。\n在执行 kcctl deploy 命令后，命令会检测您的环境是否符合安装要求，会将警告信息、安装进度等同步输出到控制台，最后在安装成功后会打印如下 KubeClipper banner：\n_ __ _ _____ _ _ | | / / | | / __ \\ (_) | |/ / _ _| |__ ___| / \\/ |_ _ __ _ __ ___ _ __ | \\| | | | '_ \\ / _ \\ | | | | '_ \\| '_ \\ / _ \\ '__| | |\\ \\ |_| | |_) | __/ \\__/\\ | | |_) | |_) | __/ | \\_| \\_/\\__,_|_.__/ \\___|\\____/_|_| .__/| .__/ \\___|_| | | | | |_| |_| 系统默认管理账号：admin / Thinkbig1\n登录控制台： 打开浏览器，访问 http://SERVER_IP （通过任意一个 Server 节点均可访问）即可进入 KubeClipper 控制台\n登录命令行：\nkcctl login -H http://SERVER_IP -u admin -p Thinkbig1 大多数 kcctl 命令都依赖登录状态，因此最好在执行 cli 命令时提前登录。\n使用 kcctl 添加 agent 节点到 KubeClipper 当前 kcctl join 命令仅支持添加 agent 节点，后续会逐步支持添加 server 节点。 新加入的 agent 节点也应该统一设置免密登录，且私钥相同。\nJoin agent 节点：\nkcctl join --agent=AGENT_IPS 使用 kcctl 从 KubeClipper 中删除 agent 节点 当前 kcctl drain 命令仅支持删除 agent 节点，后续会逐步支持删除 server 节点。\nDrain agent 节点：\nkcctl drain --agent=AGENT_IPS # 强制 drain 节点，忽略错误 kcctl drain --agent=AGENT_IPS --force 如果您发现根据本文档无法成功部署 KubeClipper，请移步 KubeClipper Github Issue，提出您的意见或反馈。\n","categories":"","description":"快速部署高可用、生产就绪的 KubeClipper 集群\n","excerpt":"快速部署高可用、生产就绪的 KubeClipper 集群\n","ref":"/v1.3/docs/deployment-docs/ha-deploy/","tags":"","title":"部署高可用 KubeClipper"},{"body":"1. Go to the creation screen Log in to the Kubeclipper platform and click the button as shown in the figure to enter the cluster creation interface\n2. Configure cluster nodes Follow the text prompts to complete the steps of entering the cluster name and selecting nodes\nNote: The number of master nodes cannot be an even number.\n3. Configure cluster This step is used to configure the cluster network and components such as the database and container runtime\nSelect offline installation and fill in the address of the image repository you have built first\n4. Configure cluster storage Select nfs storage and follow the text prompts to fill in the appropriate fields\n5. Installation completed Complete all configurations to confirm installation\nInstallation is successful and the cluster is up and running\n","categories":"","description":"How to create a kubernetes cluster offline using the KC platform\n","excerpt":"How to create a kubernetes cluster offline using the KC platform\n","ref":"/v1.3/en/docs/getting-started/carete-k8s-cluster-offline/","tags":"","title":"Create kubernetes clusters offline using the kubeclipper platform"},{"body":"Kubeadm cluster hosting For a host cluster created and managed by kubeadm, kubeclipper gets the cluster and node information from the kubeconfig file and imports it into the kubeclipper platform.\nClick “Cluster Management” \u003e “Cluster Hosting” button to enter the cluster hosting page. Click “Add” button at the upper left corner. In the pop-up window of Add Provider, fill in the provider name (such as kubeadm-demo) and description, and then fill in the following information:\nRegion: The region of the cluster and node in the kubeclipper platform.\nProvider type: Select kubeadm.\nSSH: Specifies the connection method of cluster nodes. Private Key or Password can be selected. Ensure that all cluster nodes can be connected through the selected method.\nPrivate Key: enter the node user name and private key information. Password: enter the node user name and password. Cluster name: Specifies the display name on the platform and cannot be the same as any other clusters.\nKubeConfig: The KubeConfig file of the host cluster.\nClick the “OK” button to import the cluster and node into the platform. Click the provider name (kubeadm-demo) to enter the Provider detail page, where you can view the cluster under the provider and perform the following operations on the provider:\nSynchronization: Kubeclipper synchronizes cluster information every four hours. You can also click “Synchronize” to manually perform the operation. Edit: Edit the provider’s name, description, access information, and node connection method. Remove: Remove the cluster information from kubeclipper, but the cluster will not be uninstalled. Managed cluster management You can choose “Cluster Management” \u003e “Cluster” to go to the cluster list page and view the list of all clusters, including hosted clusters and local clusters. The following table lists the operations supported by different clusters:\nNote that “docker.io” will be used as image resource by default when you install external storage and other plug-ins for host clusters. If you are in an offline environment, you need to fill in the address of the accessible private registry during plug-in installation. The private registry must be added to the CRI registry of the cluster. For details, refer to CRI Registry.\nFunction Clusters created by Kubeclipper Hosted kubeadm cluster View log ✔ ✔ Retry after failed task ✔ ✔ Access Kubectl ✔ ✔ Edit ✔ ✔ Save as template ✔ ✘ CRI Registry ✔ ✔ Add/remove cluster nodes ✔ ✔ Cluster Backup and Recovery ✔ ✔ Version Upgrade ✔ ✘ Delete cluster ✔ ✘ Remove cluster (provider) / ✔ Reset status ✔ ✔ Cluster plugin management ✔ ✔ Update cluster certificate ✔ ✔ View kubeconfig file ✔ ✔ ","categories":"","description":"For kubernetes clusters running outside the kubeclipper platform, you can host them within the kubeclipper platform for management. The current version supports host kubeadmin clusters.\n","excerpt":"For kubernetes clusters running outside the kubeclipper platform, you can host them within the kubeclipper platform for management. The current version supports host kubeadmin clusters.\n","ref":"/v1.3/en/docs/tutorials/cluster-hosting/","tags":"","title":"Cluster hosting"},{"body":"Kubeadm 集群托管 托管由 kubeadm 创建和管理的 kubernetes 集群，kubeclipper 会通过 kubeconfig 文件，获取集群和节点信息，并导入到 kubeclipper 平台中。\n点击“集群管理”\u003e“集群托管”按钮进入集群托管页面，点击左上角“添加”按钮，在添加提供商弹窗中，填写提供商名称（如 kubeadm-demo）和描述后，填写以下信息：\n区域：提供商下的集群和节点在 kubeclipper 平台中的所属区域。 提供商类型：选择 kubeadm。 节点连接方式：集群节点的连接方式，可以选择“私钥”或者“密码”，您需要确保可以通过所选方式 ssh 到集群节点。 选择“私钥”，需要输入节点用户名和私钥信息。 选择“密码”，需要输入节点用户名和密码信息。 集群名称：作为在本平台的展示名称，不能与其他集群重复。 KubeConfig：托管集群的 KubeConfig 文件。 填写完成后点击“确定”按钮，将集群和节点导入到平台中。点击提供商名称（kubeadm-demo），进入提供商详情页，您可以查看提供商下的集群，并对提供商执行以下操作：\n同步：系统将每隔4小时定期同步集群信息，您也可以点击“同步”按钮手动执行。 编辑：编辑提供商的名称、描述、访问信息、节点连接方式。 移除：移除集群信息，但集群不会被卸载。 托管集群管理 您可以点击“集群管理”\u003e“集群”进入集群列表页面，查看包括托管集群和本地集群在内的所有集群列表，对于不同集群，支持的管理操作如下表所示：\n注意：托管的 kubeadm 集群安装插件或外接存储的镜像来源会默认使用 docker.io，如果您处于离线环境，需要在安装插件时填写可访问的离线镜像仓库地址，该离线镜像仓库的地址需要预先添加到 cri 的私有镜像仓库配置，参见 CRI 镜像仓库配置 。\n功能 Kubeclipper 创建的集群 托管的 kubeadm 集群 查看日志 ✔︎ ✔︎ 任务失败后重试 ✔︎ ✔︎ 访问 kubectl ✔︎ ✔︎ 编辑 ✔︎ ✔︎ 保存为模版 ✔︎ ✘ CRI 镜像仓库配置 ✔︎ ✔︎ 添加/移除集群节点 ✔︎ ✔︎ 备份和恢复管理 ✔︎ ✔︎ 版本升级 ✔︎ ✘ 删除集群 ✔︎ ✘ 移除集群（提供商） / ✔︎ 重置状态 ✔︎ ✔︎ 集群插件管理 ✔︎ ✔︎ 更新集群证书 ✔︎ ✔︎ 查看 kubeconfig ✔︎ ✔︎ ","categories":"","description":"对于在 kubeclipper 平台外运行的 kubernetes 集群，您可以托管至 kubeclipper 平台内进行管理，当前版本支持托管 kubeadmin 集群。\n","excerpt":"对于在 kubeclipper 平台外运行的 kubernetes 集群，您可以托管至 kubeclipper 平台内进行管理，当前版本支持托管 kubeadmin 集群。\n","ref":"/v1.3/docs/tutorials/cluster-hosting/","tags":"","title":"集群托管"},{"body":"","categories":"","description":"Deploying the sample\n","excerpt":"Deploying the sample\n","ref":"/v1.3/en/docs/deployment-docs/","tags":"","title":"Deployment docs"},{"body":"","categories":"","description":"部署示例\n","excerpt":"部署示例\n","ref":"/v1.3/docs/deployment-docs/","tags":"","title":"部署文档"},{"body":"1. 进入创建界面 登录 Kubeclipper 平台后点击如图所示按钮，进入集群创建界面\n2. 配置集群节点 按照文字提示完成输入集群名称、选择节点等步骤\n注意: master 节点数量不能为偶数\n3. 配置集群 此步骤用于配置集群网络以及数据库、容器运行时等组件\n选择离线安装并填写首先搭建好的镜像仓库地址\n4. 配置存储 选择 nfs 存储，按照文字提示填写相应内容\n5. 安装完成 完成所有配置确认安装\n安装成功，集群正常运行\n","categories":"","description":"如何使用 KC 平台离线创建 kubernetes 集群\n","excerpt":"如何使用 KC 平台离线创建 kubernetes 集群\n","ref":"/v1.3/docs/getting-started/carete-k8s-cluster-offline/","tags":"","title":"使用 Kubeclipper 离线创建 kubernetes 集群"},{"body":"View Cluster operations On the cluster details page, click the \"Operation Log\" tab to see the cluster operation records. Click the \"ViewLog\" button on the right side to inspect the detailed logs of all steps and nodes in the pop-up window. Click the step name on the left to inspect the detailed log of the execution steps.\nDuring the execution of cluster operations, you can inspect real-time log updates to trace the operation execution. For operations that failed to execute, you can also locate error by red dot under the step name, and troubleshoot the cause of the operation failure.\nTry again after failed task If the task failed but you do not need to modify the task parameters after troubleshooting, you can click “Retry” on the right of the operation record to retry the task at the breakpoint.\nNote: The retry operation is not universal. You need to determine the cause of the task failure by yourself.\nAfter cluster operation (such as creation, restoration, and upgrade) failure, the cluster status may be displayed as “xx failed” and other operations cannot be performed. If the operation can not be retrayed successflly. You need to refer to the O\u0026M document to manually rectify the cluster error, and click More \u003e Cluster Status \u003e Reset Status to reset the cluster to normal status.\nAccess Kubectl The Kubernetes command-line tool, kubectl, allows you to run commands on Kubernetes clusters. You can use kubectl to deploy applications, inspect and manage cluster resources, view logs, and more.\nClick \"More\" \u003e \"Connect Terminal\" in the cluster operation, and you can execute the kubectl commands in the cluster kuebectl pop-up window.\nCluster Settings Edit You can click More \u003e Cluster Settings \u003e Edit on the right of the cluster list to edit the cluster description, backup space, external access IP address, and cluster label information.\nSave as template You can click More \u003e Cluster Settings \u003e Save as Template on the right of the cluster list to save the cluster settings as a template and use it to creat new clusters with similar configurations.\nCRI Registry Docker and Containerd use dockerhub as the default registry. If you need to use other private registry (especially self-signed https registries or http registries), you need to configure CRI registry.\nClick “More” \u003e “Cluster Settings” \u003e “CRI Registry” on the right of the cluster page. In the pop-up window configure the required private registry. You can select an existing registry on the platform or temporarily enter the address of a registry. For a self-signed https or http registry, it is recommended to add the registry information on the Cluster Management \u003e Registry in advance.\nCluster node management On the \"Nodes list\" page of the cluster detail page, you can view the list of nodes in the cluster, specification, status and role information of the nodes.\nAdd cluster node When the cluster load is high, you can add nodes to the cluster to expand capacity. Adding nodes operation does not affect the running services.\nOn the cluster detail page, under the Node List tab, click the \"AddNode\" button on the left, select the available nodes in the pop-up window, set the node labels, and click the \"OK\" button. The current version only supports adding worker nodes.\nRemove cluster node On the cluster detail page, under the Node List tab, you can remove a node by clicking the \"Remove\" button on the right of the node. The current version only supports removing worker nodes.\nNote: To remove cluster nodes, you need to pay attention to security issues in production to avoid application interruptions.\nCluster Backup and Recovery The backup of kubernetes cluster by KubeClipper backs up the data of ETCD database, and kubernetes resource object, such as namespaces, deployments, configMaps. The files and data generated by the resource itself are not backed up. For example, the data and files generated by the mysql pod will not be backed up. Similarly, the files under the PV object are not backed up, only the pv object is backed up. The backup function provided by KubeClipper is hot backup, which does not affect cluster usage. While KubeClipper strongly disapproves of backing up during the \"busy period\" of the cluster.\nCreate a backup space Before performing a backup operation, you need to set a backup space for the cluster, that is, set the storage location of the backup files. The storage type of the backup space can be FS storage or S3 storage . Tack the node local storage , NFS storage and MINIO storage as examples:\nNode local storage (only for AIO experimental clusters): Create a storage directory. Connect to the cluster master node terminal ( refer to Connect Nodes Terminal ) and use the mkdir command to create the \"/root/backup\" directory in the master node.\nCreate a backup space. Click \"Cluster Management\" \u003e \"backup space\" to enter the backup space list page, click the \"Create\" button in the upper left corner, in the Create pop-up window, enter \"Backup Space Name\", such as \"local\", select \"StorageType\" as \"FS\", fill in \"backupRootDir\" as \"/root/backup\".\nSet up the cluster backup space. When creating a cluster, select \"backup space\" as \"local\" on the \"Cluster Config\" page, or edit an existing cluster and select \"local\" as the \"backup space\".\nNote: Using a local node to store backup files does not require the introduction of external storage. The disadvantage is that if the local node is damaged, the backup files will also be lost, so it is strongly disapproved in a production environment .\nNFS： Prepare NFS storage. Prepare an NFS service and create a directory on the NFS server to store backup files, such as \"/data/kubeclipper/cluster-backups\".\nMount the storage directory. Connect the cluster master node terminal ( refer to Connect node Terminal ), use the mkdir command to create the \"/data/kubeclipper/cluster-backups\" directory in each master node, and mount it to the /data/kubeclipper/cluster-backups directory of the NFS server.\nCommand example:\nmount -t nfs {NFS\\_IP}:/data/kubeclipper/cluster-backups /opt/kubeclipper/cluster-backups -o proto = tcp -o nolock Create a backup space. Click \"Cluster Management\" \u003e \"Backup Space\" to enter the backup space list page, click the \"Create\" button in the upper left corner, in the Create pop-up window, enter \"Backup Space Name\", such as \"nfs\", select \"StorageType\" as \"FS\", fill in \"backupRootDir\" as \"/opt/kubeclipper/cluster-backups\".\nSet up the cluster backup space. When creating a cluster, select \"backup space\" as \"nfs\" on the \"Cluster Config\" page, or edit an existing cluster and select \"nfs\" as the \"backup space\".\nMINIO： Prepare MINIO storage. Build MINIO services, refer to the official website https://docs.min.io/docs/minio-quickstart-guide.html for the deployment process, or use existing MINIO services.\nCreate a backup space. Click \"Cluster Management\" \u003e \"Backup Space\" to enter the backup space list page, click the \"Create\" button in the upper left corner, in the Create window, enter \"Backup Space Name\", such as \"minio\", select \"Storage Type\" as \"S3\", fill in \"bucket name\", such as \"kubeclipper-backups\", the bucket will be automatically created by kubeclipper, fill in the IP and port number of the MINIO storage service in the first step in \"Endpoint\", fill in the service username and password, click the \"OK\" button.\nSet up the cluster backup space. When creating a cluster, select \"backup space\" as \"minio\" on the \"Cluster Config\" page, or edit an existing cluster and select \"minio\" as the \"backup space\".\nYou can view the list and details of all backup spaces on the \"Cluster Management\"\u003e\"backup spaces\" page and perform the following operations:\nEdit: Edit the backup space description, and the username/password of the S3 type backup space.\nDelete: Delete the backup space. If there are backup files under the backup space, deletion is not allowed.\nCluster backup You can back up your cluster ETCD data by clicking the \"More\" \u003e “Backup and recovery” \u003e \"Backup Cluster\" button in the cluster operation.\nYou can view all backup files of the cluster under the Backup tab on the cluster detail page, and you can perform the following operations for backups:\nEdit: Edit the backup description.\nRestore: Performs a cluster restore operation to restore the cluster to the specified backup state.\nDelete: Deletes the backup file.\nScheduled backup You can also create a scheduled backup task for the cluster, click the \"More\" \u003e “Backup and recovery” \u003e \"Scheduled Backup\" button in the cluster operation, in the Scheduled Backup pop-up window, enter the scheduled backup name, execution type ( repeat / onlyonce) and execution time, and set the number of valid backups for a repeat scheduled backups, and click the \"OK\" button.\nkubeClipper will perform backup tasks for the cluster at the execution time you set, and the backup file will be automatically named \"Cluster Name - Scheduled Backup Name - Random Code\". For repeat scheduled backups, when the number of backup files exceeds the number of valid backup files, kubeClipper will automatically delete the earlier backup files.\nAfter the scheduled backup task is added, you can view the scheduled backup task information on the \"Scheduled Backup\" tab of the cluster detail page, and you can also view the backup files generated by the scheduled backup on the \"Backup\" tab.\nFor scheduled backup tasks, you can also perform the following operations:\nEdit: Edit the execution time of the scheduled backup task and the number of valid backups for repeat scheduled backups.\nEnable/Disable: Disabled scheduled backup tasks are temporarily stopped.\nDelete: Delete a scheduled backup task.\nRestore Cluster If you perform restore operation while the cluster is running, KubeClipper will perform overlay recovery on the cluster, that is, the ETCD data in the backup file, overwriting the existing data .\nYou can click the \"Restore\" button on the right side of the backup under the Backup tab of the cluster detail page; or click the \"More\" \u003e “Backup and recovery” \u003e \"Restore Cluster\" button in the cluster operation, and select the backup to be restored in the Restore Cluster pop-up window. The current cluster can be restored to the specified backup state.\nNote: After the kubernetes version of the cluster is upgraded, it will no longer be possible to restore the cluster to the pre-upgrade backup version.\nCluster Status Cluster version upgrade If the cluster version does not meet the requirements, you can upgrade the kubernetes version of the cluster. Similar to creating a cluster, you need to prepare the configuration package required and the kubernetes image of the target version, upload them to the specified location. For details, refer to Prepare to Create a Cluster.\nClick the \"More\" \u003e “Cluster status” \u003e \"Cluster Upgrade\" button of the cluster operation. In the cluster upgrade pop-up window, select the installation method and registry, and select the target upgrade version. The installation method and the configuration of the kubernetes version are the same as those of creating a cluster. For details, please refer to Cluster Configuration Guide.\nCluster upgrades can be performed across minor versions, but upgrades skipped over later versions are not supported. For example, you can upgrade from v1.20.2 to v1.20.13, or from v1.20.x to v1.21.x, but not from v1.20.x to v1.22.x. For version 1.23.x, upgrading to version 1.24.x is not currently supported.\nThe cluster upgrade operation may take a long time. You can view the operation log on the cluster detail page to track the cluster upgrade status.\nDelete cluster You can click “More” \u003e “Cluster Status” \u003e “Delete Cluster” on the right of the cluster list to delete the cluster.\nNote that after the cluster is deleted, it cannot be restored. You must perform this operation with great caution. If the cluster is connected to an external storage device, the volumes in the storage class whose reclaim policy is “Retain” will be retained. You can access them in other ways or manually delete them. Volumes in the storage class whose reclaim policy is “Delete” will be automatically deleted when the cluster is deleted.\nReset the status After cluster operation (such as creation, restoration, and upgrade) failure, the cluster status may be displayed as “xx failed” and other operations cannot be performed. If the operation can not be retrayed successflly. You need to refer to the O\u0026M document to manually rectify the cluster error, and click More \u003e Cluster Status \u003e Reset Status to reset the cluster to normal status.\nCluster plugin management In addition to installing plugins when creating a cluster, you can also install plugins for a running cluster. Taking the installation of storage plugins as an example, click the \"More\" \u003e “plugin management”\u003e\"Add Storage\" button in the cluster operation to enter the Add Storage page. You can install NFS plugins for the cluster. The installation configuration is the same as the configuration in cluster creation.\nFor installed plugins, you can view the plugin information on the cluster detail page, and perform the following operations:\nSave as Template: Save the plugin information as a template for use by other clusters Remove plug-in: Uninstalls the cluster plug-in. Cluster certificate management Update cluster certificate The default validity period of the kubernetes cluster certificate is one year. You can view the certificate expiration time in the basic information on the cluster detail page. You can also view the certificate expiration notification in the cluster list the day before the certificate expires. To update the cluster certificate, click “More” \u003e “Cluster Certificate” \u003e “Update Cluster Certificate” in the cluster operation to update all cluster certificates.\nView kubeconfig file You can click “More” \u003e “Cluster Certificate” \u003e “View KubeConfig File” button in the cluster operation to view the cluster kubeconfig file, or click “Download” button in the pop-up window to download the kubeconfig file.\n","categories":"","description":"kubeclipper supports full lifecycle management for Kubernetes clusters.\n","excerpt":"kubeclipper supports full lifecycle management for Kubernetes clusters.\n","ref":"/v1.3/en/docs/tutorials/cluster-management/","tags":"","title":"Cluster management"},{"body":"集群操作日志查看 在集群详情页面，点击“操作日志”标签页，可以查看集群操作日志列表。点击操作日志右侧“查看日志”按钮，可以在弹窗中查看全部步骤和节点的详细日志。点击左侧步骤名称，可查看执行步骤详细的日志输出。\n在集群操作执行过程中，点击查看日志，您可以实时查看到日志更新来跟踪操作执行情况。对于执行失败的任务，您也可以通过查看日志，找到红色圆点标注的执行步骤和节点，快速定位错误，排查操作失败原因。\n任务失败后重试 对于任务执行失败，但排查错误原因后不需要修改任务参数的情况，您可以点击操作日志右侧的“从断点处重试”按钮，从断点处重新执行任务。\n注意：重试操作并不是万能的，您需要自行判断任务执行失败的原因，处理后如果不需要更改执行任务时填写的参数，就可以点击重试按钮，从错误处重新开始执行。\n集群操作（如创建、恢复、升级等）执行失败，可能会导致集群状态显示为“xx失败”并无法正常执行其他操作，如果从断点处重试也无法执行成功，您可以参考运维文档，手动修复集群问题。问题修复后，您可以点击集群右侧“更多”\u003e“集群状态”\u003e“重置状态”按钮，重置集群至正常状态。\n访问集群 kubectl Kubectl 是 Kubernetes 命令行工具，您可以用它在 Kubernetes 集群上运行命令。Kubectl 可用于部署应用、查看和管理集群资源、查看日志等。\n您可以访问运行中集群的 kubectl，点击集群操作中的“更多“\u003e”访问 kubectl”，就可以在集群 kuebectl 弹窗中执行 kubectl 命令行操作。\n集群设置 编辑 您可以点击集群列表右侧“更多”\u003e“集群设置”\u003e“编辑”按钮，编辑集群描述、备份空间、外部访问 IP、集群标签信息。\n保存为模版 您可以点击集群列表右侧“更多”\u003e“集群设置”\u003e“保存为模版”按钮，将集群信息保存为模版，以便再次创建相似配置的集群时使用。\nCRI 镜像仓库配置 Docker 和 Containerd 使用 dockerhub 作为默认镜像仓库，如果您需要使用其他镜像仓库（特别是使用自签名 https 仓库或者 http 仓库），您需要配置 CRI 镜像仓库。\n点击集群右侧“更多”\u003e“集群设置”\u003e“CRI 镜像仓库”按钮，在 CRI 镜像仓库弹窗中，配置您需要的镜像仓库，您可以选择平台中已存在的镜像仓库，也可以临时填写镜像仓库地址。对于自签名 https 仓库或者 http 仓库，建议您先在“集群管理”\u003e“镜像仓库”页面添加仓库信息，再在此操作中配置。\n集群节点管理 在集群详情页的“节点”列表页面，您可以查看当前集群中的节点列表，节点的规格、状态和角色信息。\n添加集群节点 当集群负载较高时，您可以通过为集群添加节点来达到主动扩容的目的，添加新的“节点”不会影响现有的业务的运行。\n在集群详情页的节点列表标签页下，点击左侧的“添加节点”按钮，在弹窗中选择可用节点，设置节点标签，点击“确认”按钮。当前版本仅支持添加工作节点。\n移除集群节点 在集群详情页的节点列表标签页下，您可以点击节点右侧的 “移除”按钮移除节点。当前版本仅支持移除工作节点。\n注意：移除集群节点，您需要注意生产中的安全问题，避免应用发生中断。\n集群备份和恢复 KubeClipper 对 kubernetes 集群的备份主要为备份 ETCD 数据库数据，以及 kubernetes 的资源对象备份，如 namespace，deployment、configMap。对资源自身产生的文件和数据不做备份，例如对集群中运行的 mysql pod，该 mysql pod 产生的数据和文件，不会为之备份，同理，文件类的 pv 对象下的文件，也不做备份，仅仅备份 pv 这个对象。KubeClipper 提供的备份功能是热备份，备份期间不影响集群的使用。KubeClipper 虽然不反对在集群 “繁忙期” 备份，但也强烈不赞成在集群 “繁忙期” 备份。\n创建备份空间 执行备份之前，您需要先为集群设置备份空间，即设置备份文件的存储位置。备份空间的存储类型可以是 FS 存储或 S3 存储，下面以节点本地存储、NFS 存储和 MINIO 存储为例：\n节点本地存储（仅适用单节点实验集群）： 创建存储目录。连接集群 master 节点终端（可参见连接节点终端），使用 mkdir 命令，在 master 节点中创建“/root/backup”目录。 创建备份空间。点击“集群管理”\u003e“备份空间”，进入备份空间列表页，点击右上角“创建”按钮，在创建备份空间弹窗中，输入“备份空间名称”，如 “local”，选择“存储类型”为 “FS”，填写“备份路径”，如 “/root/backup”。 设置集群备份空间。创建集群时，在“集群配置”页面选择“备份空间”为 “local”，或者编辑已有集群，在编辑集群弹窗中的“备份空间”中选择 “local”。 注意：使用本地节点存储备份文件，不需要引入外部存储，缺点是如果本地节点遭到破坏，备份文件也会丢失，所以强烈不赞成在生产环境中使用。\nNFS： 准备 NFS 存储。准备一台 NFS 服务，并在 NFS 服务器上创建一个用于存放备份文件的目录，如 “/data/kubeclipper/cluster-backups”。\n挂载存储目录。连接集群 master 节点终端（可参见连接节点终端），使用 mkdir 命令，在每个 master 节点中创建 “/data/kubeclipper/cluster-backups” 目录，并 mount 到 NFS 服务器的 /data/kubeclipper/cluster-backups 目录即可。\n命令示例：\nmount -t nfs { NFS_IP }:/data/kubeclipper/cluster-backups /opt/kubeclipper/cluster-backups -o proto=tcp -o nolock 创建备份空间。点击“集群管理”\u003e“备份空间”，进入备份空间列表页，点击右上角“创建”按钮，在创建备份空间弹窗中，输入“备份空间名称”，如 “nfs”，选择“存储类型”为 “FS”，填写“备份路径”为 “/opt/kubeclipper/cluster-backups”。\n设置集群备份空间。创建集群时，在“集群配置”页面选择“备份空间”为 “nfs”，或者编辑已有集群，在编辑集群弹窗中的“备份空间”中选择 “nfs”。\nMINIO： 准备 MINIO 存储。搭建 MINIO 服务，部署过程参考官网 https://docs.min.io/docs/minio-quickstart-guide.html，也可以使用已有 MINIO 服务。 创建备份空间。点击“集群管理”\u003e“备份空间”，进入备份空间列表页，点击右上角“创建”按钮，在创建备份空间弹窗中，输入“备份空间名称”，如 “minio”，选择“存储类型”为 “S3”，填写 “bucket 名称”，如 “kubeclipper-backups”，该 bucket 将由 kubeclipper 自动创建，“Endpoint” 中填写第一步 MINIO 存储服务的 ip 和端口号，填写服务用户名和密码，点击“确定”按钮。 设置集群备份空间。创建集群时，在“集群配置”页面选择“备份空间”为 “minio”，或者编辑已有集群，在编辑集群弹窗中的“备份空间”中选择“minio”。 您可以在“集群管理”的“备份空间”页面查看所有备份空间列表和详细信息，并执行以下操作：\n编辑：编辑备份空间描述，和 S3 类型备份空间的用户名/密码。\n删除：删除备份空间，备份空间下存在备份文件的，不允许删除。\n集群备份 您可以点击集群操作中的“更多”\u003e“备份和恢复”\u003e “集群备份”按钮，备份集群 ETCD 数据。\n您可以在集群详情页面的备份标签页下，查看当前集群的所有备份文件，还可以对备份执行以下操作：\n编辑：编辑备份描述。\n恢复：执行集群恢复操作，将集群恢复至指定备份状态。\n删除：删除指定备份文件。\n定时备份 您也可以为集群创建定时备份，点击集群操作中的“更多”\u003e“备份和恢复”\u003e “定时备份”按钮，在定时备份弹窗中，输入定时备份名称、执行类型（重复执行/仅执行一次）和执行时间，并为重复执行的定时备份设置有效备份个数，点击“确认”按钮。\nkubeClipper 会在您设置的执行时间为集群执行备份任务，备份文件会自动命名为“集群名称-定时备份名称-随机码”，对于重复执行的定时备份，kubeClipper 会在该定时任务下的备份文件超过有效备份个数时，自动删除超出个数的较早的备份文件。\n定时备份添加完成后，可以在集群详情页的“定时备份”标签页查看定时备份信息，也可以在“备份”标签页查看定时备份产生的备份文件。\n对于定时备份任务，您还可以执行以下操作：\n编辑：编辑定时备份任务执行时间，和重复执行的定时备份的有效备份个数。\n启用/禁用：禁用的定时备份任务将暂时停止执行。\n删除：删除定时备份任务。\n集群备份恢复 如果您在集群正常运行期间执行恢复操作，则 KubeClipper 将对该集群进行覆盖式恢复，就是备份文件里面的 etcd 数据，覆盖现有的数据。\n您可以在集群详情页的备份标签页下，点击备份右侧的 “恢复”按钮；或点击集群操作中的“更多”\u003e“备份和恢复”\u003e“恢复集群”按钮，在恢复集群弹窗中选择需要恢复的备份，可以将当前集群恢复至指定备份状态。\n注意：集群升级后，将无法再恢复到升级前版本的备份。\n集群状态 集群版本升级 如当集群版本不满足需要，您可以为集群升级 kubernetes 版本。与创建集群一样，您需要准备好集群版本所需配置包和目标版本的 kubernetes 镜像并上传至指定位置，详情参见创建集群准备工作。\n点击集群操作的“更多”\u003e“集群状态”\u003e “集群升级”按钮，在集群升级弹窗中选择安装方式和镜像仓库，选择升级的目标版本，升级的安装方式和 kubernetes 版本的配置与创建集群相同，详情参见集群配置指南。\n集群升级可以跨小版本，但不支持略过次版本的升级，如您可以从 v1.20.2 升级到 v1.20.13，或由 v1.20.x 升级到 v1.21.x，但不支持从 v1.20.x 升级到 v1.22.x。对于 1.23.x 版本，暂不支持升级到 1.24.x 版本。\n升级集群操作可能需要较长时间，您可以在集群详情页面查看操作日志，跟踪集群升级状态。\n删除集群 您可以点击集群列表右侧“更多”\u003e“集群状态”\u003e “删除”按钮，删除集群。\n注意删除后集群不可恢复。请谨慎操作。如果集群对接了外部存储，回收策略为“保留”的存储类中的数据卷会被保留，您可以通过其他方式访问，或手动删除；回收策略为“删除”的存储类中的数据卷，会在删除集群时自动删除。\n重置状态 集群操作（如创建、恢复、升级等）执行失败，可能会导致集群状态显示为“xx失败”并无法正常执行其他操作，如果从断点处重试也无法执行成功，您可以参考运维文档，手动修复集群问题。问题修复后，您可以点击集群右侧“更多”\u003e“集群状态”\u003e“重置状态”按钮，重置集群至正常状态。\n集群插件管理 除了在创建集群时安装插件，您也可以为运行中的集群安装存储和其他自定义插件。以安装存储插件为例，点击集群操作中的“更多”\u003e“插件管理”\u003e“添加存储项”按钮，进入添加存储项页面，可以为集群安装存储插件，安装配置与创建集群中的配置相同。\n对于已安装的插件，您可以在集群详情页查看插件信息，并执行以下操作：\n保存为模版：将插件信息保存为模版，以便为其他集群使用。\n移除插件：卸载集群插件。\n集群证书管理 更新集群证书 kubernetes 集群证书默认有效期为一年，您可以在集群详情页的基本信息中查看证书过期时间，证书过期前一天，您也可以在集群列表看到证书过期的提醒。更新集群证书，您可以点击集群操作中的“更多”\u003e“集群证书”\u003e“更新集群证书”按钮，更新集群全部证书。\n获取 kubeconfig 文件 您可以点击集群操作中的“更多”\u003e“集群证书”\u003e“查看 kubeconfig 文件”按钮，查看集群 kubeconfig 文件，也可以点击弹窗中的“下载”按钮，下载 kubeconfig 文件。\n","categories":"","description":"kubeclipper 支持对 kubernetes 集群的全生命周期管理。\n","excerpt":"kubeclipper 支持对 kubernetes 集群的全生命周期管理。\n","ref":"/v1.3/docs/tutorials/cluster-management/","tags":"","title":"集群管理"},{"body":"问题复现 安装 v1.2.1 版本的 kcctl\ncurl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_VERSION=v1.2.1 bash - 通过 kcctl deploy 命令部署 KubeClipper 集群\n# 安装 AIO 环境 kcctl deploy 通过 kcctl create cluster 命令创建 kubernetes 集群\n# 需要先登录 kcctl login --host http://127.0.0.1 --username admin --password Thinkbig1 # 创建集群 kcctl create cluster --name test --master 192.168.10.98 --untaint-master 登录 KubeClipper 管理界面，查看创建集群操作日志，日志显示在安装 cni 过程中发现下载 calico v3.21.2 404 无法找到\n问题修复 PR 提交已经合并到了 master，PR：https://github.com/kubeclipper/kubeclipper/commit/7e6eb0ed199ff1cb00fde0c2624c62cdc5ca0b9c\n但 v1.2.1 已经发布了，按照发版规范无法在该版本打补丁，需要等到后续 v1.2.2 发布解决，因此我们提供一种临时方案来解决这个问题。\n解决方法 制作离线资源包 下载 calico v3.21.2 的安装包，打包为指定格式的离线资源包\n# 创建资源目录 mkdir -pv calico/v3.21.2/amd64 # 下载 v3.21.2 版本的 calico wget -P calico/v3.21.2/amd64 https://oss.kubeclipper.io/packages/calico/v3.21.2/amd64/images.tar.gz wget -P calico/v3.21.2/amd64 https://oss.kubeclipper.io/packages/calico/v3.21.2/amd64/manifest.json # 压缩文件为指定命令 tar -zcvf calico-v3.21.2-amd64.tar.gz calico 推送离线资源包\n# 推送 kcctl resource push --pkg calico-v3.21.2-amd64.tar.gz --type cni # 验证 kcctl resource list|grep v3.21.2 如果在执行 kcctl resource push 报了如下错误： 解决方法如下：\n编辑 /root/.kc/deploy-config.yaml 文件。 找到 ssh 配置项，添加 pkFile 字段配置，值为当前服务器的 ssh 公钥文件的绝对路径。\n通过命令行安装 kubernetes 集群，在 KubeClipper 管理后台查看操作日志\nkcctl create cluster --name test --master 192.168.10.98 --untaint-master 查看 kubernetes 集群 pods 运行状态\nkubectl get pods -A ","categories":["FAQ"],"description":"在 v1.2.1 版本（包括 v1.2.1）之前，使用 `kcctl create cluster` 命令创建集群功能，会发生创建失败错误，以下提供一种临时的解决方法。 在 v1.2.1 版本之后，我们修复了该问题。\n","excerpt":"在 v1.2.1 版本（包括 v1.2.1）之前，使用 `kcctl create cluster` 命令创建集群功能，会发生创建失败错误，以下提供一种临时的解决方法。 在 v1.2.1 版本之后，我们修复了该问题。\n","ref":"/v1.3/docs/faq/kcctl-create-cluster-error/","tags":["kcctl","create","cluster"],"title":"通过 kcctl 命令创建集群错误"},{"body":"","categories":"","description":"用户使用手册\n","excerpt":"用户使用手册\n","ref":"/v1.3/docs/tutorials/","tags":"","title":"使用手册"},{"body":"","categories":"","description":"User manual\n","excerpt":"User manual\n","ref":"/v1.3/en/docs/tutorials/","tags":"","title":"Tutorials"},{"body":"","categories":"","description":"Q\u0026A\n","excerpt":"Q\u0026A\n","ref":"/v1.3/en/docs/faq/","tags":"","title":"FAQ"},{"body":"Region management KubeClipper supports multi-region management. That is, all nodes and clusters managed by the platform are divided into physical or logical regions. On the Region Management page, you can view all regions in the platform. Click a region name to enter the region detail page, and you can view the clusters and nodes in the region.\nNode management The platform supports multi-region management, that is, the owning region of all nodes managed by the platform. On the Region Management page, you can view all regions managed by the platform. Click a region name to go to the region details page, where you can view the list of all clusters and nodes in the region.\nOn the \"Node Info\" page, you can view the list of all nodes managed in the platform, node specifications, status and other information. Click the node name to enter the node detail page, you can view detailed node basic information and system information.\nThe node status in KubeClipper represents the management status of the node by kc-gent. Under normal circumstances, the node status is displayed as \"Ready\". When the node is out of contact for 4 minutes (within 10s of the error time), the status will be updated to \"Unknown\". Nodes with unknown status cannot perform any operations, nor can they create clusters or add/remove nodes to clusters.\nAdd node When deploying KubeClipper, you can add the initial server nodes which are used to deploy KubeClipper's own services, and agent nodes which are used to deploy kubernetes clusters. In a KubeClipper environment for experimentation or development, you can add a server node as an agent node at the same time. However, if it is used for a production environment, it is recommended not to reuse the server node as an agent node.\nYou can also use the kcctl join command to add agent nodes to KubeClipper, and mark a region for each agent node. The region can be a physical or logical location. You can use nodes in the same region to create a kubernetes cluster, but cannot use nodes across regions to create a cluster. Nodes in unmarked regions belong to the default region.\nCommand line example:\nkcctl join --agent beijing:1.2.3.4 --agent shanghai:2.3.4.5 Remove node When you no longer need some nodes, you can use the kcctl drain command to remove nodes from the platform.\nCommand line example:\nkcctl drain --agent 192.168.10.19 Connect Terminal On the node list page, you can click the \"Connect Terminal\" button on the right side of the target node, enter the node port and username password information in the pop-up window, access the node SSH console and execute command tasks.\nEnable/disable a node You can click the Disable button on the right side of the node to temporarily disable the node. The node in the disabled state cannot be created or added to the cluster.\n","categories":"","description":"KubeClipper supports node and cluster management across multiple regions.\n","excerpt":"KubeClipper supports node and cluster management across multiple regions.\n","ref":"/v1.3/en/docs/tutorials/node-management/","tags":"","title":"Node management"},{"body":"区域管理 平台支持多区域管理，即为平台管理的所有节点和集群进行物理或逻辑的区域划分。您可以在“区域管理”页面查看平台内管理的所有区域，点击区域名称，进入区域详情页面，可以查看区域下的所有集群列表和节点列表。\n节点管理 您可以在“节点信息”页面查看平台中管理的全部节点列表，和节点规格、状态等信息。点击节点名称进入节点详情页面，可以查看详细的节点基本信息和系统信息。\nKubeClipper 中的节点状态表示 kc-agent 对节点的管理状态。正常情况下，节点状态显示为“就绪”，当节点失联4分钟（误差时间 10s 内）后，状态会更新为“未知”，未知状态的节点无法进行任何操作，也无法创建集群或为集群添加/移除节点。\n添加节点 在部署 KubeClipper 时，您就可以添加初始的 server 节点和 agent 节点，其中，server 节点用于部署 KubeClipper 自身服务，agent 节点可用于部署 kubernetes 集群。在用于实验或开发的 KubeClipper 环境，您可以将 server 节点同时添加为 agent 节点。但如果用于生产环境，建议不要将 server 节点复用为 agent 节点。\n您也可以使用 kcctl join 命令为 KubeClipper 添加 agent 节点。同时，您可以为每个 agent 节点标记一个区域，区域可以是物理的或逻辑的位置，您可以使用同一区域的节点创建 kubernetes 集群，但不可以使用跨区域的节点创建集群。未标记区域的节点默认属于 default 区域。\n命令行示例：\nkcctl join --agent beijing:1.2.3.4 --agent shanghai:2.3.4.5 移除节点 当您不再需要某些节点，可以使用 kcctl drain 命令将节点从平台中移除。\n命令行示例：\nkcctl drain --agent 192.168.10.19 连接终端 在节点列表页面，您可以点击目标节点右侧的“连接终端”按钮，在连接终端的弹窗中输入节点端口和用户名密码信息后，访问节点 SSH 控制台并执行命令。\n启用/禁用节点 您可以点击节点右侧“禁用”按钮暂时禁用节点，禁用状态下的节点不允许创建或添加到集群。\n","categories":"","description":"KubeClipper 支持跨区域的节点和集群管理。\n","excerpt":"KubeClipper 支持跨区域的节点和集群管理。\n","ref":"/v1.3/docs/tutorials/node-management/","tags":"","title":"节点管理"},{"body":"","categories":"","description":"常见问题记录\n","excerpt":"常见问题记录\n","ref":"/v1.3/docs/faq/","tags":"","title":"常见问题"},{"body":"Create user After installing KubeClipper, you need to create a user of a desired role. Initially, there is only one user, admin, by default, with the platform administrator role.\nClick \"Access Control\" \u003e \"Users\" to enter the user management page, click the \"Create User\" button in the upper left corner, fill in the user name, password, alias name and other information in the pop-up window, specify the user role, and click the \"OK\" button. The four initial roles in the system are as follows:\nplatform-admin: Platform administrator, with the authority to set platform, cluster management, user management, audit, etc.\ncluster-manager: Cluster administrator, with all cluster management permissions.\niam-manager: User administrator, with all user management permissions.\nPlatform-view: Platform read-only user, with all platform viewing permissions.\nAfter the user is created, you can view the user details and login logs on the user detail page and perform the following operations:\nEdit: Edit user alias, role, mobile phone number, email information.\nEdit Password: Edit the user login password.\nDelete: Delete the user.\nCreate a custom role In addition to system initial roles, you can also create customized roles to meet business needs.\nClick \"Access Control\" \u003e \"Roles\" to enter the role management page. You can click the \"Create Role\" button in the upper left corner to create a custom role.\nOn the Create Role page, you need to fill in the role name and description, and check the permissions required. Some permissions depend on other permissions. When these permissions are selected, the dependent permissions will be automatically selected.\nAfter creating a customized role, you can view the basic role information, role permission list, authorized user list on the role detail page, and perform the following operations:\nEdit: Edit the custom role description.\nEdit permissions: Edit permissions of the customized role.\nDelete: To delete a customized role, make sure that no user is using the role to be deleted.\nAccess to external users External users can log in to KubeClipper via the OIDC protocol .\nFirst, the platform administrator needs to log in to the platform server node and insert the following information under “authentication” in the kubeclipepr-server.yaml file:\noauthOptions: identityProviders: - name: keycloak type: OIDC mappingMethod: auto provider: clientID: kc clientSecret: EErn729BB1bKawdRtnZgrqj9Bx0]mzUs issuer: http://172.20.163.233:7777/auth/realms/kubeclipper scopes: - openid - email redirectURL: http://{kc-console-address}/oatuh2/redirect/{IDP-Name} Under \"provider\", you need to fill in the clientID , clientSecret , and issuer information of your OAuth2 service, taking keycloack as an example, as shown in the figure below.\nRedirectURL example: http://172.0.0.90/oauth2/redirect/keycloack\nOAuth2 users can log in to the KubeClipper platform by following these steps:\nClick the \"OAuth2 Login\" button on the login page, enter the OAuth2 login page, fill in the username and password to enter the KubeClipper platform. When logging in for the first time, you will not be able to access the platform because you have not been granted any permission.\nThe platform administrator or other user with user management rights log in to KubeClipper, find the target OAuth2 user on the user management page, and set the user role by editing the user information.\nThe OAuth2 user repeats the first step, logs in to KubeClipper, and accesses the platform normally.\n","categories":"","description":"KubeClipper access control usage guide.\n","excerpt":"KubeClipper access control usage guide.\n","ref":"/v1.3/en/docs/tutorials/access-control/","tags":"","title":"Access control"},{"body":"用户管理 安装 KubeClipper 之后，您需要创建所需角色的用户。一开始，系统默认只有一个用户 admin，具有平台管理员角色。\n点击“访问控制”\u003e“用户”，进入用户管理页面，点击左上角“创建用户”按钮，在弹窗中填写用户名、密码、手机号码、邮箱等信息，并指定用户角色，点击“确认”按钮。系统内置四个角色如下：\n平台管理员：拥有集群管理、用户管理、审计管理等全部平台查看和操作权限。\n集群管理员：拥有所有集群管理权限。\n用户管理员：拥有所有用户管理权限。\n平台只读用户：拥有全部平台查看权限。\n用户创建完成后，您可以在用户详情页面查看用户详情信息，并执行以下操作：\n编辑：编辑用户别名、角色、手机号、邮箱信息。\n编辑密码：编辑用户登录密码。\n删除：删除用户。\n在用户详情页面，您还可以查看当前用户的登录日志列表。\n您可以编辑平台 /etc/kubeclipper-server/kubeclipper-server.yaml 文件，根据需要设置用户登录日志的最大保留条数和保留期限，每个用户的登录日志超过最大条数后，超过最大条数和期限的日志会被自动删除，仅超过保留期限但未超过最大条数的日志数据将不会被删除。\n自定义角色 除了系统内置角色，您也可以创建自定义角色，以满足业务需要。\n点击“访问控制”\u003e“角色”，进入角色管理页面，您可以点击左上角“创建角色”按钮，创建自定义角色。\n在创建角色页面，您需要填写角色名称和描述，并勾选自定义角色所需权限，一些权限依赖于其他权限，在选择这些权限时，将自动选中依赖的权限。\n创建自定义角色完成后，您可以在角色详情页面查看角色基本信息、角色权限列表、授权用户列表，并对自定义角色执行以下操作：\n编辑：编辑自定义角色别名。\n编辑权限：编辑自定义角色下的权限。\n删除：删除自定义角色，需确保没有用户使用待删除角色。\n接入外部用户 KubeClipper 可以通过 OIDC 协议使用外部用户登录。\n首先，平台管理员需要登录平台 server 节点，在 kubeclipepr-server.yaml 文件中的 authentication 下插入以下信息：\noauthOptions: identityProviders: - name: keycloak type: OIDC mappingMethod: auto provider: clientID: kc clientSecret: EErn729BB1bKawdRtnZgrqj9Bx0]mzUs issuer: http://172.20.163.233:7777/auth/realms/kubeclipper scopes: - openid - email redirectURL: http://{kc-console-address}/oatuh2/redirect/{IDP-Name} 其中，“provider” 下需要您填写自己的 OAuth2 服务的clientID、clientSecret、issuer信息，以 keycloack 为例，如下图所示。\nredirectURL示例：http://172.0.0.90/oauth2/redirect/keycloack\nOAuth2 用户可以通过以下步骤访问和使用 KubeClipper 平台：\n点击登录页的 “OAuth2 登录”按钮，进入 OAuth2 登录页面，输入用户名密码登录，进入 KubeClipper 平台，首次登录，您会因未被授予权限而无法访问平台。 平台管理员或其他拥有用户管理权限的用户登录 KubeClipper，在用户管理页面，找到目标 OAuth2 用户，通过编辑用户指定用户角色。 OAuth2 用户重复第一步，登录 KubeClipper，就可以正常访问平台并执行角色权限内操作。 ","categories":"","description":"KubeClipper 访问控制功能使用指南。\n","excerpt":"KubeClipper 访问控制功能使用指南。\n","ref":"/v1.3/docs/tutorials/access-control/","tags":"","title":"访问控制"},{"body":"Please check KubeClipper Community\n","categories":"","description":"How to contribute to the docs\n","excerpt":"How to contribute to the docs\n","ref":"/v1.3/en/docs/contribution-guidelines/","tags":"","title":"Contribution Guidelines"},{"body":"请查看 KubeClipper Community\n","categories":"","description":"如何为 kubeclipper 做贡献\n","excerpt":"如何为 kubeclipper 做贡献\n","ref":"/v1.3/docs/contribution-guidelines/","tags":"","title":"贡献指南"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/docs/","tags":"","title":"Documentation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/docs/","tags":"","title":"文档"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/blog/news/","tags":"","title":"News About Docsy"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/blog/releases/","tags":"","title":"New Releases"},{"body":" Welcome to Kubeclipper! Learn More Download Manage Kubernetes in the most light and simple way !\nKubeClipper is a lightweight, simple graphical interface Kubernetes cluster lifecycle management platform.\nIt is fully compatible with kubernetes, allows users to quickly deploy the kubernetes clusters required by the enterprise on customer-managed infrastructures through a friendly wizard-like Web UI, and provides continuous full life cycle management capabilities.\nSimple Provides friendly wizard graphical interface, allows beginners to quickly deploy a cluster and the required plug-ins.\nLightweight Simple architecture, few dependencies, only two command lines for platform deployment.\nProfessional Simple and professional, it supports rich cluster parameter configuration and plug-in management, meeting production-level cluster deployment requirements.\nQuick Start 1. Download kcctl 2. Deploy KubeClipper curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | sh - # In China, you can add cn env, we use registry.aliyuncs.com/google_containers instead of k8s.gcr.io curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn sh - kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) ","categories":"","description":"","excerpt":" Welcome to Kubeclipper! Learn More Download Manage Kubernetes in the most light and simple way !\nKubeClipper is a lightweight, simple graphical interface Kubernetes cluster lifecycle management …","ref":"/v1.3/en/","tags":"","title":"Kubeclipper"},{"body":" 欢迎来到 Kubeclipper ! 了解更多 免费下载 用最简单的方式管理 Kubernetes !\nKubeClipper 是一个轻量级、易用的图形界面 Kubernetes 集群生命周期管理平台。\n它与 kubernetes 完全兼容，用户可以通过友好的向导式 Web UI，在客户管理的基础架构上快速部署企业所需的 kubernetes 集群，并提供持续的全生命周期管理能力。\n易使用 友好的向导式图形化界面，初学者也可以快速部署一个生产级集群并安装所需插件。\n极轻量 架构简单，少依赖，平台部署仅需两个命令行。\n生产级 易用性和专业性兼顾，支持丰富的集群参数配置和插件管理，满足生产级集群部署需求。\n立即体验 1. 下载 kcctl 2. 部署 KubeClipper curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | sh - # In China, you can add cn env, we use registry.aliyuncs.com/google_containers instead of k8s.gcr.io curl -sfL https://oss.kubeclipper.io/get-kubeclipper.sh | KC_REGION=cn sh - kcctl deploy [--user root] (--passwd SSH_PASSWD | --pk-file SSH_PRIVATE_KEY) ","categories":"","description":"","excerpt":" 欢迎来到 Kubeclipper ! 了解更多 免费下载 用最简单的方式管理 Kubernetes !\nKubeClipper 是一个轻量级、易用的图形界面 Kubernetes 集群生命周期管理平台。\n它与 kubernetes 完全兼容，用户可以通过友好的向导式 Web UI，在客户管理的基础架构上快速部署企业所需的 kubernetes 集群，并提供持续的全生命周期管理能力。\n易使用 友好 …","ref":"/v1.3/","tags":"","title":"Kubeclipper"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/cluster/","tags":"","title":"cluster"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/create/","tags":"","title":"create"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/categories/faq/","tags":"","title":"FAQ"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/kcctl/","tags":"","title":"kcctl"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/","tags":"","title":"Tags"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/aio/","tags":"","title":"aio"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/tags/aio/","tags":"","title":"aio"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/tags/docs/","tags":"","title":"docs"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/categories/quickstart/","tags":"","title":"QuickStart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/categories/quickstart/","tags":"","title":"QuickStart"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/tags/sample/","tags":"","title":"sample"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/tags/sample/","tags":"","title":"sample"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/tags/","tags":"","title":"Tags"},{"body":"This is a typical blog post that includes images.\nThe front matter specifies the date of the blog post, its title, a short description that will be displayed on the blog landing page, and its author.\nIncluding images Here’s an image (featured-sunset-get.png) that includes a byline and a caption.\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\nThe front matter of this post specifies properties to be assigned to all image resources:\nresources: - src: \"**.{png,jpg}\" title: \"Image #:counter\" params: byline: \"Photo: Riona MacNamara / CC-BY-CA\" To include the image in a page, specify its details like this:\nFetch and scale an image in the upcoming Hugo 0.43. Photo: Riona MacNamara / CC-BY-CA\nThe image will be rendered at the size and byline specified in the front matter.\n","categories":"","description":"The Docsy Hugo theme lets project maintainers and contributors focus on content, not on reinventing a website infrastructure from scratch","excerpt":"The Docsy Hugo theme lets project maintainers and contributors focus on content, not on reinventing a website infrastructure from scratch","ref":"/v1.3/en/blog/2018/10/06/easy-documentation-with-docsy/","tags":"","title":"Easy documentation with Docsy"},{"body":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","categories":"","description":"","excerpt":"This is the blog section. It has two categories: News and Releases.\nFiles in these directories will be listed in reverse chronological order.\n","ref":"/v1.3/en/blog/","tags":"","title":"Docsy Blog"},{"body":"Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\nFirst Header This is a normal paragraph following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nBacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nLorem markdownum tuta hospes stabat; idem saxum facit quaterque repetito occumbere, oves novem gestit haerebat frena; qui. Respicit recurvam erat: pignora hinc reppulit nos aut, aptos, ipsa.\nMeae optatos passa est Epiros utiliter Talibus niveis, hoc lata, edidit. Dixi ad aestum.\nHeader 2 This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nHeader 3 This is a code block following a header. Header 4 This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header. Header 5 This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header. Header 6 What Follows A table A header A table A header A table A header There’s a horizontal rule above and below this.\nHere is an unordered list:\nSalt-n-Pepa Bel Biv DeVoe Kid ‘N Play And an ordered list:\nMichael Jackson Michael Bolton Michael Bublé And an unordered task list:\nCreate a sample markdown document Add task lists to it Take a vacation And a “mixed” task list:\nSteal underpants ? Profit! And a nested list:\nJackson 5 Michael Tito Jackie Marlon Jermaine TMNT Leonardo Michelangelo Donatello Raphael Definition lists can be used with Markdown syntax. Definition terms are bold.\nName Godzilla Born 1952 Birthplace Japan Color Green Tables should have bold headings and alternating shaded rows.\nArtist Album Year Michael Jackson Thriller 1982 Prince Purple Rain 1984 Beastie Boys License to Ill 1986 If a table is too wide, it should scroll horizontally.\nArtist Album Year Label Awards Songs Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin’ Somethin’, Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\"; Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. Inline code inside table cells should still be distinguishable.\nLanguage Code Javascript var foo = \"bar\"; Ruby foo = \"bar\"{ Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nComponents Alerts This is an alert. Note: This is an alert with a title. This is a successful alert. This is a warning! Warning! This is a warning with a title! Sizing Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nParameters available Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing pixels Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing rem Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMemory Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nRAM to use Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMore is better Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsed RAM Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis is the final element on the page and there should be no margin below this. ","categories":"","description":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","ref":"/v1.3/en/blog/2018/10/06/the-second-blog-post/","tags":"","title":"The second blog post"},{"body":"Text can be bold, italic, or strikethrough. Links should be blue with no underlines (unless hovered over).\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs. There should be whitespace between paragraphs.\nThere should be no margin above this first sentence.\nBlockquotes should be a lighter gray with a border along the left side in the secondary color.\nThere should be no margin below this final sentence.\nFirst Header This is a normal paragraph following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nBacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nOn big screens, paragraphs and headings should not take up the full container width, but we want tables, code blocks and similar to take the full width.\nLorem markdownum tuta hospes stabat; idem saxum facit quaterque repetito occumbere, oves novem gestit haerebat frena; qui. Respicit recurvam erat: pignora hinc reppulit nos aut, aptos, ipsa.\nMeae optatos passa est Epiros utiliter Talibus niveis, hoc lata, edidit. Dixi ad aestum.\nHeader 2 This is a blockquote following a header. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nHeader 3 This is a code block following a header. Header 4 This is an unordered list following a header. This is an unordered list following a header. This is an unordered list following a header. Header 5 This is an ordered list following a header. This is an ordered list following a header. This is an ordered list following a header. Header 6 What Follows A table A header A table A header A table A header There’s a horizontal rule above and below this.\nHere is an unordered list:\nSalt-n-Pepa Bel Biv DeVoe Kid ‘N Play And an ordered list:\nMichael Jackson Michael Bolton Michael Bublé And an unordered task list:\nCreate a sample markdown document Add task lists to it Take a vacation And a “mixed” task list:\nSteal underpants ? Profit! And a nested list:\nJackson 5 Michael Tito Jackie Marlon Jermaine TMNT Leonardo Michelangelo Donatello Raphael Definition lists can be used with Markdown syntax. Definition terms are bold.\nName Godzilla Born 1952 Birthplace Japan Color Green Tables should have bold headings and alternating shaded rows.\nArtist Album Year Michael Jackson Thriller 1982 Prince Purple Rain 1984 Beastie Boys License to Ill 1986 If a table is too wide, it should scroll horizontally.\nArtist Album Year Label Awards Songs Michael Jackson Thriller 1982 Epic Records Grammy Award for Album of the Year, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Selling Album, Grammy Award for Best Engineered Album, Non-Classical Wanna Be Startin’ Somethin’, Baby Be Mine, The Girl Is Mine, Thriller, Beat It, Billie Jean, Human Nature, P.Y.T. (Pretty Young Thing), The Lady in My Life Prince Purple Rain 1984 Warner Brothers Records Grammy Award for Best Score Soundtrack for Visual Media, American Music Award for Favorite Pop/Rock Album, American Music Award for Favorite Soul/R\u0026B Album, Brit Award for Best Soundtrack/Cast Recording, Grammy Award for Best Rock Performance by a Duo or Group with Vocal Let’s Go Crazy, Take Me With U, The Beautiful Ones, Computer Blue, Darling Nikki, When Doves Cry, I Would Die 4 U, Baby I’m a Star, Purple Rain Beastie Boys License to Ill 1986 Mercury Records noawardsbutthistablecelliswide Rhymin \u0026 Stealin, The New Style, She’s Crafty, Posse in Effect, Slow Ride, Girls, (You Gotta) Fight for Your Right, No Sleep Till Brooklyn, Paul Revere, Hold It Now, Hit It, Brass Monkey, Slow and Low, Time to Get Ill Code snippets like var foo = \"bar\"; can be shown inline.\nAlso, this should vertically align with this and this.\nCode can also be shown in a block element.\nfoo := \"bar\"; bar := \"foo\"; Code can also use syntax highlighting.\nfunc main() { input := `var foo = \"bar\";` lexer := lexers.Get(\"javascript\") iterator, _ := lexer.Tokenise(nil, input) style := styles.Get(\"github\") formatter := html.New(html.WithLineNumbers()) var buff bytes.Buffer formatter.Format(\u0026buff, style, iterator) fmt.Println(buff.String()) } Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this. Inline code inside table cells should still be distinguishable.\nLanguage Code Javascript var foo = \"bar\"; Ruby foo = \"bar\"{ Small images should be shown at their actual size.\nLarge images should always scale down and fit in the content container.\nComponents Alerts This is an alert. Note: This is an alert with a title. This is a successful alert. This is a warning! Warning! This is a warning with a title! Sizing Add some sections here to see how the ToC looks like. Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nParameters available Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing pixels Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsing rem Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMemory Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nRAM to use Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nMore is better Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nUsed RAM Bacon ipsum dolor sit amet t-bone doner shank drumstick, pork belly porchetta chuck sausage brisket ham hock rump pig. Chuck kielbasa leberkas, pork bresaola ham hock filet mignon cow shoulder short ribs biltong.\nThis is the final element on the page and there should be no margin below this. ","categories":"","description":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","excerpt":"A short lead description about this content page. Text here can also be **bold** or _italic_ and can even be split over multiple paragraphs.\n","ref":"/v1.3/en/blog/2018/01/04/another-great-release/","tags":"","title":"Another Great Release"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/v1.3/en/community/","tags":"","title":"Community"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/categories/examples/","tags":"","title":"Examples"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/categories/placeholders/","tags":"","title":"Placeholders"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/search/","tags":"","title":"Search Results"},{"body":"","categories":"","description":"","excerpt":"","ref":"/v1.3/en/tags/test/","tags":"","title":"test"},{"body":"Comming soon…\n","categories":"","description":"","excerpt":"Comming soon…\n","ref":"/v1.3/blog/","tags":"","title":"KubeClipper Blog"},{"body":" ","categories":"","description":"","excerpt":" ","ref":"/v1.3/community/","tags":"","title":"社区"}]